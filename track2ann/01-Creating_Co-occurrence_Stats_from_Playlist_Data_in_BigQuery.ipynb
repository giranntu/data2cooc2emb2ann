{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Co-occurrence Statistics from Playlist Data in BigQuery\n",
    "\n",
    "This tutorial shows how to compute **Co-occurrence statistics** for items occurring together in the same context. In this example, the items are music tracks, and the context is playlists. \n",
    "\n",
    "The statistics we compute for each co-occurring pair is the [Pointwise mutual information](https://en.wikipedia.org/wiki/Pointwise_mutual_information) (pmi), which is used by the [Swivel](https://arxiv.org/pdf/1602.02215.pdf) algorithm for learning embeddings.\n",
    "\n",
    "<img src=\"tabular2cooc.png\" width=\"600\" height=\"400\"/>\n",
    "\n",
    "The dataset we use is in [Google BigQuery](https://bigquery.cloud.google.com/table/bigquery-samples:playlists.playlist?pli=1), and we use [Apache Beam](https://beam.apache.org/get-started/beam-overview/) to implement the pmi computation process. Beam can run at scale using [Cloud Dataflow](https://cloud.google.com/dataflow/).\n",
    "\n",
    "The following are the steps of this tutorial:\n",
    "\n",
    "\n",
    "1. Exract data from BigQuery to filesystem as CSV.\n",
    "2. Compute Co-occurrence statistics and store them as TFRecord files.\n",
    "3. Read statistics in the TFRecords using tf.data APIs\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/ksalama/data2cooc2emb2ann/blob/master/track2ann/01-Creating_Co-occurrence_Stats_from_Playlist_Data_in_BigQuery.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using COLAB\n",
    "try:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import apache_beam as beam\n",
    "import tensorflow as tf\n",
    "from google.cloud import bigquery\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'ksalama-cloudml'\n",
    "REGION = 'europe-west1'\n",
    "WORKSPACE = './workspace'\n",
    "DATA_DIR = '{}/data'.format(WORKSPACE)\n",
    "COOC_DIR = '{}/cooc'.format(WORKSPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.io.gfile.exists(WORKSPACE):\n",
    "    print(\"Removing {} contents...\".format(WORKSPACE))\n",
    "    tf.io.gfile.rmtree(WORKSPACE)\n",
    "\n",
    "print(\"Creating workspace: {}\".format(WORKSPACE))\n",
    "tf.io.gfile.makedirs(WORKSPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "* 430K+ playlists\n",
    "* 850K+ tracks\n",
    "* 120M+ entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khalidsalama/Technology/GoogleCloud/GCP-Github/kfp-components/google/tf_hub/tabular2cooc/venv/lib/python3.6/site-packages/google/auth/_default.py:66: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>playlist_id</th>\n",
       "      <th>track_id</th>\n",
       "      <th>track_title</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>album_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7712011</td>\n",
       "      <td>3636957</td>\n",
       "      <td>Maggot Brain (Think It Ain't Illegal Yet)</td>\n",
       "      <td></td>\n",
       "      <td>Cosmic Funkers, Vol. 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3072830</td>\n",
       "      <td>3636958</td>\n",
       "      <td>Freak Of The Week</td>\n",
       "      <td></td>\n",
       "      <td>Cosmic Funkers, Vol. 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1855902</td>\n",
       "      <td>3636982</td>\n",
       "      <td>The Pot Head Pixies</td>\n",
       "      <td></td>\n",
       "      <td>Magick Invocations, Vol. 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4557029</td>\n",
       "      <td>3637094</td>\n",
       "      <td>My Baby Just Cares For Me</td>\n",
       "      <td></td>\n",
       "      <td>Lady Blue Part 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5345222</td>\n",
       "      <td>3637082</td>\n",
       "      <td>He's Got The Whole World In His Hands</td>\n",
       "      <td></td>\n",
       "      <td>Lady Blue Part 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6289202</td>\n",
       "      <td>3637082</td>\n",
       "      <td>He's Got The Whole World In His Hands</td>\n",
       "      <td></td>\n",
       "      <td>Lady Blue Part 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>645572</td>\n",
       "      <td>3637082</td>\n",
       "      <td>He's Got The Whole World In His Hands</td>\n",
       "      <td></td>\n",
       "      <td>Lady Blue Part 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1914713</td>\n",
       "      <td>3637096</td>\n",
       "      <td>Don't Let Me Be Misunderstood</td>\n",
       "      <td></td>\n",
       "      <td>Lady Blue Part 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>9570584</td>\n",
       "      <td>3637112</td>\n",
       "      <td>Ain't Got No; I Got Life</td>\n",
       "      <td></td>\n",
       "      <td>Live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3597625</td>\n",
       "      <td>3637241</td>\n",
       "      <td>Leaving On A Jet Plane</td>\n",
       "      <td></td>\n",
       "      <td>Turn Your Radio On</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   playlist_id  track_id                                track_title  \\\n",
       "0      7712011   3636957  Maggot Brain (Think It Ain't Illegal Yet)   \n",
       "1      3072830   3636958                          Freak Of The Week   \n",
       "2      1855902   3636982                        The Pot Head Pixies   \n",
       "3      4557029   3637094                  My Baby Just Cares For Me   \n",
       "4      5345222   3637082      He's Got The Whole World In His Hands   \n",
       "5      6289202   3637082      He's Got The Whole World In His Hands   \n",
       "6       645572   3637082      He's Got The Whole World In His Hands   \n",
       "7      1914713   3637096              Don't Let Me Be Misunderstood   \n",
       "8      9570584   3637112                   Ain't Got No; I Got Life   \n",
       "9      3597625   3637241                     Leaving On A Jet Plane   \n",
       "\n",
       "  artist_name                 album_title  \n",
       "0                  Cosmic Funkers, Vol. 1  \n",
       "1                  Cosmic Funkers, Vol. 1  \n",
       "2              Magick Invocations, Vol. 1  \n",
       "3                        Lady Blue Part 1  \n",
       "4                        Lady Blue Part 1  \n",
       "5                        Lady Blue Part 1  \n",
       "6                        Lady Blue Part 1  \n",
       "7                        Lady Blue Part 2  \n",
       "8                                    Live  \n",
       "9                      Turn Your Radio On  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = '''\n",
    "    SELECT  \n",
    "      id AS playlist_id,\n",
    "      tracks_data_id AS track_id,\n",
    "      tracks_data_title AS track_title, \n",
    "      tracks_data_artist_name AS artist_name, \n",
    "      tracks_data_album_title AS album_title \n",
    "    FROM \n",
    "      `bigquery-samples.playlists.playlist`\n",
    "    WHERE\n",
    "      tracks_data_id > 0\n",
    "    AND \n",
    "        tracks_data_title IS NOT NULL\n",
    "    AND \n",
    "        tracks_data_artist_name IS NOT NULL\n",
    "    AND\n",
    "       tracks_data_album_title IS NOT NULL \n",
    "    LIMIT 10\n",
    "'''\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "query_job = bq_client.query(query)\n",
    "results = query_job.result().to_dataframe()\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract data from BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the query generation method\n",
    "We are going to extract the entries for tracks that appeared more than N playlist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query(min_occurrence=50, limit=None):\n",
    "    query = '''\n",
    "    WITH data\n",
    "    AS\n",
    "    (\n",
    "      SELECT  \n",
    "        id AS playlist_id,\n",
    "        tracks_data_id AS track_id\n",
    "      FROM\n",
    "        `bigquery-samples.playlists.playlist`\n",
    "      WHERE\n",
    "        tracks_data_id > 0\n",
    "    ),\n",
    "\n",
    "    frequeny_tracks\n",
    "    AS\n",
    "    (\n",
    "      SELECT \n",
    "        track_id\n",
    "      FROM\n",
    "        data\n",
    "      GROUP BY \n",
    "        track_id\n",
    "      HAVING \n",
    "        count(playlist_id) >= {}\n",
    "    ),\n",
    "\n",
    "    playlists_with_frequent_tracks\n",
    "    AS\n",
    "    (\n",
    "      SELECT DISTINCT \n",
    "        p.playlist_id\n",
    "      FROM\n",
    "        data p\n",
    "      JOIN\n",
    "        frequeny_tracks t\n",
    "      ON\n",
    "        p.track_id = t.track_id\n",
    "    )\n",
    "\n",
    "    SELECT \n",
    "      d.playlist_id,\n",
    "      d.track_id\n",
    "    FROM\n",
    "      data d\n",
    "    JOIN\n",
    "      playlists_with_frequent_tracks p\n",
    "    ON\n",
    "      d.playlist_id = p.playlist_id\n",
    "  \n",
    "    '''.format(min_occurrence)\n",
    "\n",
    "    if limit:\n",
    "        query += \"LIMIT {}\".format(limit)\n",
    "\n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Data extraction pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_extraction_pipeline(args):\n",
    "    \n",
    "    def _to_csv(bq_row):\n",
    "        return \"{},{}\".format(bq_row['playlist_id'], bq_row['track_id'])\n",
    "\n",
    "    source_query = args['source_query']\n",
    "    sink_data_location = args['sink_data_location']\n",
    "    runner = args['runner']\n",
    "    \n",
    "    pipeline_options = beam.options.pipeline_options.GoogleCloudOptions(**args)\n",
    "\n",
    "    \n",
    "    with beam.Pipeline(runner, options=pipeline_options) as pipeline:\n",
    "        (pipeline \n",
    "         | \"Read from BigQuery\">> beam.io.Read(beam.io.BigQuerySource(query = source_query, use_standard_sql = True))\n",
    "         | 'Convert to CSV' >> beam.Map(_to_csv)\n",
    "         | \"Write to file\" >> beam.io.WriteToText(\n",
    "                    file_path_prefix = sink_data_location+\"/data\", \n",
    "                    file_name_suffix=\".csv\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run data extraction pipeline locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = 'DirectRunner'\n",
    "job_name = 'playlist-data-extraction-{}'.format(datetime.utcnow().strftime('%y%m%d-%H%M%S'))\n",
    "\n",
    "source_query = generate_query(min_occurrence=50, limit=1000000)\n",
    "\n",
    "args = {\n",
    "    'job_name': job_name,\n",
    "    'runner': runner,\n",
    "    'source_query': source_query,\n",
    "    'sink_data_location': DATA_DIR,\n",
    "    'project': PROJECT_ID,\n",
    "}\n",
    "print(\"Pipeline args are set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = datetime.utcnow() \n",
    "print(\"Running data extraction pipeline...\")\n",
    "run_extraction_pipeline(args)\n",
    "print(\"Pipeline is done.\")\n",
    "time_end = datetime.utcnow() \n",
    "time_elapsed = time_end - time_start\n",
    "print(\"Execution elapsed time: {} seconds\".format(time_elapsed.total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data-00000-of-01000.csv data-00334-of-01000.csv data-00668-of-01000.csv\r\n",
      "data-00001-of-01000.csv data-00335-of-01000.csv data-00669-of-01000.csv\r\n",
      "data-00002-of-01000.csv data-00336-of-01000.csv data-00670-of-01000.csv\r\n",
      "data-00003-of-01000.csv data-00337-of-01000.csv data-00671-of-01000.csv\r\n",
      "data-00004-of-01000.csv data-00338-of-01000.csv data-00672-of-01000.csv\r\n",
      "data-00005-of-01000.csv data-00339-of-01000.csv data-00673-of-01000.csv\r\n",
      "data-00006-of-01000.csv data-00340-of-01000.csv data-00674-of-01000.csv\r\n",
      "data-00007-of-01000.csv data-00341-of-01000.csv data-00675-of-01000.csv\r\n",
      "data-00008-of-01000.csv data-00342-of-01000.csv data-00676-of-01000.csv\r\n",
      "data-00009-of-01000.csv data-00343-of-01000.csv data-00677-of-01000.csv\r\n",
      "data-00010-of-01000.csv data-00344-of-01000.csv data-00678-of-01000.csv\r\n",
      "data-00011-of-01000.csv data-00345-of-01000.csv data-00679-of-01000.csv\r\n",
      "data-00012-of-01000.csv data-00346-of-01000.csv data-00680-of-01000.csv\r\n",
      "data-00013-of-01000.csv data-00347-of-01000.csv data-00681-of-01000.csv\r\n",
      "data-00014-of-01000.csv data-00348-of-01000.csv data-00682-of-01000.csv\r\n",
      "data-00015-of-01000.csv data-00349-of-01000.csv data-00683-of-01000.csv\r\n",
      "data-00016-of-01000.csv data-00350-of-01000.csv data-00684-of-01000.csv\r\n",
      "data-00017-of-01000.csv data-00351-of-01000.csv data-00685-of-01000.csv\r\n",
      "data-00018-of-01000.csv data-00352-of-01000.csv data-00686-of-01000.csv\r\n",
      "data-00019-of-01000.csv data-00353-of-01000.csv data-00687-of-01000.csv\r\n",
      "data-00020-of-01000.csv data-00354-of-01000.csv data-00688-of-01000.csv\r\n",
      "data-00021-of-01000.csv data-00355-of-01000.csv data-00689-of-01000.csv\r\n",
      "data-00022-of-01000.csv data-00356-of-01000.csv data-00690-of-01000.csv\r\n",
      "data-00023-of-01000.csv data-00357-of-01000.csv data-00691-of-01000.csv\r\n",
      "data-00024-of-01000.csv data-00358-of-01000.csv data-00692-of-01000.csv\r\n",
      "data-00025-of-01000.csv data-00359-of-01000.csv data-00693-of-01000.csv\r\n",
      "data-00026-of-01000.csv data-00360-of-01000.csv data-00694-of-01000.csv\r\n",
      "data-00027-of-01000.csv data-00361-of-01000.csv data-00695-of-01000.csv\r\n",
      "data-00028-of-01000.csv data-00362-of-01000.csv data-00696-of-01000.csv\r\n",
      "data-00029-of-01000.csv data-00363-of-01000.csv data-00697-of-01000.csv\r\n",
      "data-00030-of-01000.csv data-00364-of-01000.csv data-00698-of-01000.csv\r\n",
      "data-00031-of-01000.csv data-00365-of-01000.csv data-00699-of-01000.csv\r\n",
      "data-00032-of-01000.csv data-00366-of-01000.csv data-00700-of-01000.csv\r\n",
      "data-00033-of-01000.csv data-00367-of-01000.csv data-00701-of-01000.csv\r\n",
      "data-00034-of-01000.csv data-00368-of-01000.csv data-00702-of-01000.csv\r\n",
      "data-00035-of-01000.csv data-00369-of-01000.csv data-00703-of-01000.csv\r\n",
      "data-00036-of-01000.csv data-00370-of-01000.csv data-00704-of-01000.csv\r\n",
      "data-00037-of-01000.csv data-00371-of-01000.csv data-00705-of-01000.csv\r\n",
      "data-00038-of-01000.csv data-00372-of-01000.csv data-00706-of-01000.csv\r\n",
      "data-00039-of-01000.csv data-00373-of-01000.csv data-00707-of-01000.csv\r\n",
      "data-00040-of-01000.csv data-00374-of-01000.csv data-00708-of-01000.csv\r\n",
      "data-00041-of-01000.csv data-00375-of-01000.csv data-00709-of-01000.csv\r\n",
      "data-00042-of-01000.csv data-00376-of-01000.csv data-00710-of-01000.csv\r\n",
      "data-00043-of-01000.csv data-00377-of-01000.csv data-00711-of-01000.csv\r\n",
      "data-00044-of-01000.csv data-00378-of-01000.csv data-00712-of-01000.csv\r\n",
      "data-00045-of-01000.csv data-00379-of-01000.csv data-00713-of-01000.csv\r\n",
      "data-00046-of-01000.csv data-00380-of-01000.csv data-00714-of-01000.csv\r\n",
      "data-00047-of-01000.csv data-00381-of-01000.csv data-00715-of-01000.csv\r\n",
      "data-00048-of-01000.csv data-00382-of-01000.csv data-00716-of-01000.csv\r\n",
      "data-00049-of-01000.csv data-00383-of-01000.csv data-00717-of-01000.csv\r\n",
      "data-00050-of-01000.csv data-00384-of-01000.csv data-00718-of-01000.csv\r\n",
      "data-00051-of-01000.csv data-00385-of-01000.csv data-00719-of-01000.csv\r\n",
      "data-00052-of-01000.csv data-00386-of-01000.csv data-00720-of-01000.csv\r\n",
      "data-00053-of-01000.csv data-00387-of-01000.csv data-00721-of-01000.csv\r\n",
      "data-00054-of-01000.csv data-00388-of-01000.csv data-00722-of-01000.csv\r\n",
      "data-00055-of-01000.csv data-00389-of-01000.csv data-00723-of-01000.csv\r\n",
      "data-00056-of-01000.csv data-00390-of-01000.csv data-00724-of-01000.csv\r\n",
      "data-00057-of-01000.csv data-00391-of-01000.csv data-00725-of-01000.csv\r\n",
      "data-00058-of-01000.csv data-00392-of-01000.csv data-00726-of-01000.csv\r\n",
      "data-00059-of-01000.csv data-00393-of-01000.csv data-00727-of-01000.csv\r\n",
      "data-00060-of-01000.csv data-00394-of-01000.csv data-00728-of-01000.csv\r\n",
      "data-00061-of-01000.csv data-00395-of-01000.csv data-00729-of-01000.csv\r\n",
      "data-00062-of-01000.csv data-00396-of-01000.csv data-00730-of-01000.csv\r\n",
      "data-00063-of-01000.csv data-00397-of-01000.csv data-00731-of-01000.csv\r\n",
      "data-00064-of-01000.csv data-00398-of-01000.csv data-00732-of-01000.csv\r\n",
      "data-00065-of-01000.csv data-00399-of-01000.csv data-00733-of-01000.csv\r\n",
      "data-00066-of-01000.csv data-00400-of-01000.csv data-00734-of-01000.csv\r\n",
      "data-00067-of-01000.csv data-00401-of-01000.csv data-00735-of-01000.csv\r\n",
      "data-00068-of-01000.csv data-00402-of-01000.csv data-00736-of-01000.csv\r\n",
      "data-00069-of-01000.csv data-00403-of-01000.csv data-00737-of-01000.csv\r\n",
      "data-00070-of-01000.csv data-00404-of-01000.csv data-00738-of-01000.csv\r\n",
      "data-00071-of-01000.csv data-00405-of-01000.csv data-00739-of-01000.csv\r\n",
      "data-00072-of-01000.csv data-00406-of-01000.csv data-00740-of-01000.csv\r\n",
      "data-00073-of-01000.csv data-00407-of-01000.csv data-00741-of-01000.csv\r\n",
      "data-00074-of-01000.csv data-00408-of-01000.csv data-00742-of-01000.csv\r\n",
      "data-00075-of-01000.csv data-00409-of-01000.csv data-00743-of-01000.csv\r\n",
      "data-00076-of-01000.csv data-00410-of-01000.csv data-00744-of-01000.csv\r\n",
      "data-00077-of-01000.csv data-00411-of-01000.csv data-00745-of-01000.csv\r\n",
      "data-00078-of-01000.csv data-00412-of-01000.csv data-00746-of-01000.csv\r\n",
      "data-00079-of-01000.csv data-00413-of-01000.csv data-00747-of-01000.csv\r\n",
      "data-00080-of-01000.csv data-00414-of-01000.csv data-00748-of-01000.csv\r\n",
      "data-00081-of-01000.csv data-00415-of-01000.csv data-00749-of-01000.csv\r\n",
      "data-00082-of-01000.csv data-00416-of-01000.csv data-00750-of-01000.csv\r\n",
      "data-00083-of-01000.csv data-00417-of-01000.csv data-00751-of-01000.csv\r\n",
      "data-00084-of-01000.csv data-00418-of-01000.csv data-00752-of-01000.csv\r\n",
      "data-00085-of-01000.csv data-00419-of-01000.csv data-00753-of-01000.csv\r\n",
      "data-00086-of-01000.csv data-00420-of-01000.csv data-00754-of-01000.csv\r\n",
      "data-00087-of-01000.csv data-00421-of-01000.csv data-00755-of-01000.csv\r\n",
      "data-00088-of-01000.csv data-00422-of-01000.csv data-00756-of-01000.csv\r\n",
      "data-00089-of-01000.csv data-00423-of-01000.csv data-00757-of-01000.csv\r\n",
      "data-00090-of-01000.csv data-00424-of-01000.csv data-00758-of-01000.csv\r\n",
      "data-00091-of-01000.csv data-00425-of-01000.csv data-00759-of-01000.csv\r\n",
      "data-00092-of-01000.csv data-00426-of-01000.csv data-00760-of-01000.csv\r\n",
      "data-00093-of-01000.csv data-00427-of-01000.csv data-00761-of-01000.csv\r\n",
      "data-00094-of-01000.csv data-00428-of-01000.csv data-00762-of-01000.csv\r\n",
      "data-00095-of-01000.csv data-00429-of-01000.csv data-00763-of-01000.csv\r\n",
      "data-00096-of-01000.csv data-00430-of-01000.csv data-00764-of-01000.csv\r\n",
      "data-00097-of-01000.csv data-00431-of-01000.csv data-00765-of-01000.csv\r\n",
      "data-00098-of-01000.csv data-00432-of-01000.csv data-00766-of-01000.csv\r\n",
      "data-00099-of-01000.csv data-00433-of-01000.csv data-00767-of-01000.csv\r\n",
      "data-00100-of-01000.csv data-00434-of-01000.csv data-00768-of-01000.csv\r\n",
      "data-00101-of-01000.csv data-00435-of-01000.csv data-00769-of-01000.csv\r\n",
      "data-00102-of-01000.csv data-00436-of-01000.csv data-00770-of-01000.csv\r\n",
      "data-00103-of-01000.csv data-00437-of-01000.csv data-00771-of-01000.csv\r\n",
      "data-00104-of-01000.csv data-00438-of-01000.csv data-00772-of-01000.csv\r\n",
      "data-00105-of-01000.csv data-00439-of-01000.csv data-00773-of-01000.csv\r\n",
      "data-00106-of-01000.csv data-00440-of-01000.csv data-00774-of-01000.csv\r\n",
      "data-00107-of-01000.csv data-00441-of-01000.csv data-00775-of-01000.csv\r\n",
      "data-00108-of-01000.csv data-00442-of-01000.csv data-00776-of-01000.csv\r\n",
      "data-00109-of-01000.csv data-00443-of-01000.csv data-00777-of-01000.csv\r\n",
      "data-00110-of-01000.csv data-00444-of-01000.csv data-00778-of-01000.csv\r\n",
      "data-00111-of-01000.csv data-00445-of-01000.csv data-00779-of-01000.csv\r\n",
      "data-00112-of-01000.csv data-00446-of-01000.csv data-00780-of-01000.csv\r\n",
      "data-00113-of-01000.csv data-00447-of-01000.csv data-00781-of-01000.csv\r\n",
      "data-00114-of-01000.csv data-00448-of-01000.csv data-00782-of-01000.csv\r\n",
      "data-00115-of-01000.csv data-00449-of-01000.csv data-00783-of-01000.csv\r\n",
      "data-00116-of-01000.csv data-00450-of-01000.csv data-00784-of-01000.csv\r\n",
      "data-00117-of-01000.csv data-00451-of-01000.csv data-00785-of-01000.csv\r\n",
      "data-00118-of-01000.csv data-00452-of-01000.csv data-00786-of-01000.csv\r\n",
      "data-00119-of-01000.csv data-00453-of-01000.csv data-00787-of-01000.csv\r\n",
      "data-00120-of-01000.csv data-00454-of-01000.csv data-00788-of-01000.csv\r\n",
      "data-00121-of-01000.csv data-00455-of-01000.csv data-00789-of-01000.csv\r\n",
      "data-00122-of-01000.csv data-00456-of-01000.csv data-00790-of-01000.csv\r\n",
      "data-00123-of-01000.csv data-00457-of-01000.csv data-00791-of-01000.csv\r\n",
      "data-00124-of-01000.csv data-00458-of-01000.csv data-00792-of-01000.csv\r\n",
      "data-00125-of-01000.csv data-00459-of-01000.csv data-00793-of-01000.csv\r\n",
      "data-00126-of-01000.csv data-00460-of-01000.csv data-00794-of-01000.csv\r\n",
      "data-00127-of-01000.csv data-00461-of-01000.csv data-00795-of-01000.csv\r\n",
      "data-00128-of-01000.csv data-00462-of-01000.csv data-00796-of-01000.csv\r\n",
      "data-00129-of-01000.csv data-00463-of-01000.csv data-00797-of-01000.csv\r\n",
      "data-00130-of-01000.csv data-00464-of-01000.csv data-00798-of-01000.csv\r\n",
      "data-00131-of-01000.csv data-00465-of-01000.csv data-00799-of-01000.csv\r\n",
      "data-00132-of-01000.csv data-00466-of-01000.csv data-00800-of-01000.csv\r\n",
      "data-00133-of-01000.csv data-00467-of-01000.csv data-00801-of-01000.csv\r\n",
      "data-00134-of-01000.csv data-00468-of-01000.csv data-00802-of-01000.csv\r\n",
      "data-00135-of-01000.csv data-00469-of-01000.csv data-00803-of-01000.csv\r\n",
      "data-00136-of-01000.csv data-00470-of-01000.csv data-00804-of-01000.csv\r\n",
      "data-00137-of-01000.csv data-00471-of-01000.csv data-00805-of-01000.csv\r\n",
      "data-00138-of-01000.csv data-00472-of-01000.csv data-00806-of-01000.csv\r\n",
      "data-00139-of-01000.csv data-00473-of-01000.csv data-00807-of-01000.csv\r\n",
      "data-00140-of-01000.csv data-00474-of-01000.csv data-00808-of-01000.csv\r\n",
      "data-00141-of-01000.csv data-00475-of-01000.csv data-00809-of-01000.csv\r\n",
      "data-00142-of-01000.csv data-00476-of-01000.csv data-00810-of-01000.csv\r\n",
      "data-00143-of-01000.csv data-00477-of-01000.csv data-00811-of-01000.csv\r\n",
      "data-00144-of-01000.csv data-00478-of-01000.csv data-00812-of-01000.csv\r\n",
      "data-00145-of-01000.csv data-00479-of-01000.csv data-00813-of-01000.csv\r\n",
      "data-00146-of-01000.csv data-00480-of-01000.csv data-00814-of-01000.csv\r\n",
      "data-00147-of-01000.csv data-00481-of-01000.csv data-00815-of-01000.csv\r\n",
      "data-00148-of-01000.csv data-00482-of-01000.csv data-00816-of-01000.csv\r\n",
      "data-00149-of-01000.csv data-00483-of-01000.csv data-00817-of-01000.csv\r\n",
      "data-00150-of-01000.csv data-00484-of-01000.csv data-00818-of-01000.csv\r\n",
      "data-00151-of-01000.csv data-00485-of-01000.csv data-00819-of-01000.csv\r\n",
      "data-00152-of-01000.csv data-00486-of-01000.csv data-00820-of-01000.csv\r\n",
      "data-00153-of-01000.csv data-00487-of-01000.csv data-00821-of-01000.csv\r\n",
      "data-00154-of-01000.csv data-00488-of-01000.csv data-00822-of-01000.csv\r\n",
      "data-00155-of-01000.csv data-00489-of-01000.csv data-00823-of-01000.csv\r\n",
      "data-00156-of-01000.csv data-00490-of-01000.csv data-00824-of-01000.csv\r\n",
      "data-00157-of-01000.csv data-00491-of-01000.csv data-00825-of-01000.csv\r\n",
      "data-00158-of-01000.csv data-00492-of-01000.csv data-00826-of-01000.csv\r\n",
      "data-00159-of-01000.csv data-00493-of-01000.csv data-00827-of-01000.csv\r\n",
      "data-00160-of-01000.csv data-00494-of-01000.csv data-00828-of-01000.csv\r\n",
      "data-00161-of-01000.csv data-00495-of-01000.csv data-00829-of-01000.csv\r\n",
      "data-00162-of-01000.csv data-00496-of-01000.csv data-00830-of-01000.csv\r\n",
      "data-00163-of-01000.csv data-00497-of-01000.csv data-00831-of-01000.csv\r\n",
      "data-00164-of-01000.csv data-00498-of-01000.csv data-00832-of-01000.csv\r\n",
      "data-00165-of-01000.csv data-00499-of-01000.csv data-00833-of-01000.csv\r\n",
      "data-00166-of-01000.csv data-00500-of-01000.csv data-00834-of-01000.csv\r\n",
      "data-00167-of-01000.csv data-00501-of-01000.csv data-00835-of-01000.csv\r\n",
      "data-00168-of-01000.csv data-00502-of-01000.csv data-00836-of-01000.csv\r\n",
      "data-00169-of-01000.csv data-00503-of-01000.csv data-00837-of-01000.csv\r\n",
      "data-00170-of-01000.csv data-00504-of-01000.csv data-00838-of-01000.csv\r\n",
      "data-00171-of-01000.csv data-00505-of-01000.csv data-00839-of-01000.csv\r\n",
      "data-00172-of-01000.csv data-00506-of-01000.csv data-00840-of-01000.csv\r\n",
      "data-00173-of-01000.csv data-00507-of-01000.csv data-00841-of-01000.csv\r\n",
      "data-00174-of-01000.csv data-00508-of-01000.csv data-00842-of-01000.csv\r\n",
      "data-00175-of-01000.csv data-00509-of-01000.csv data-00843-of-01000.csv\r\n",
      "data-00176-of-01000.csv data-00510-of-01000.csv data-00844-of-01000.csv\r\n",
      "data-00177-of-01000.csv data-00511-of-01000.csv data-00845-of-01000.csv\r\n",
      "data-00178-of-01000.csv data-00512-of-01000.csv data-00846-of-01000.csv\r\n",
      "data-00179-of-01000.csv data-00513-of-01000.csv data-00847-of-01000.csv\r\n",
      "data-00180-of-01000.csv data-00514-of-01000.csv data-00848-of-01000.csv\r\n",
      "data-00181-of-01000.csv data-00515-of-01000.csv data-00849-of-01000.csv\r\n",
      "data-00182-of-01000.csv data-00516-of-01000.csv data-00850-of-01000.csv\r\n",
      "data-00183-of-01000.csv data-00517-of-01000.csv data-00851-of-01000.csv\r\n",
      "data-00184-of-01000.csv data-00518-of-01000.csv data-00852-of-01000.csv\r\n",
      "data-00185-of-01000.csv data-00519-of-01000.csv data-00853-of-01000.csv\r\n",
      "data-00186-of-01000.csv data-00520-of-01000.csv data-00854-of-01000.csv\r\n",
      "data-00187-of-01000.csv data-00521-of-01000.csv data-00855-of-01000.csv\r\n",
      "data-00188-of-01000.csv data-00522-of-01000.csv data-00856-of-01000.csv\r\n",
      "data-00189-of-01000.csv data-00523-of-01000.csv data-00857-of-01000.csv\r\n",
      "data-00190-of-01000.csv data-00524-of-01000.csv data-00858-of-01000.csv\r\n",
      "data-00191-of-01000.csv data-00525-of-01000.csv data-00859-of-01000.csv\r\n",
      "data-00192-of-01000.csv data-00526-of-01000.csv data-00860-of-01000.csv\r\n",
      "data-00193-of-01000.csv data-00527-of-01000.csv data-00861-of-01000.csv\r\n",
      "data-00194-of-01000.csv data-00528-of-01000.csv data-00862-of-01000.csv\r\n",
      "data-00195-of-01000.csv data-00529-of-01000.csv data-00863-of-01000.csv\r\n",
      "data-00196-of-01000.csv data-00530-of-01000.csv data-00864-of-01000.csv\r\n",
      "data-00197-of-01000.csv data-00531-of-01000.csv data-00865-of-01000.csv\r\n",
      "data-00198-of-01000.csv data-00532-of-01000.csv data-00866-of-01000.csv\r\n",
      "data-00199-of-01000.csv data-00533-of-01000.csv data-00867-of-01000.csv\r\n",
      "data-00200-of-01000.csv data-00534-of-01000.csv data-00868-of-01000.csv\r\n",
      "data-00201-of-01000.csv data-00535-of-01000.csv data-00869-of-01000.csv\r\n",
      "data-00202-of-01000.csv data-00536-of-01000.csv data-00870-of-01000.csv\r\n",
      "data-00203-of-01000.csv data-00537-of-01000.csv data-00871-of-01000.csv\r\n",
      "data-00204-of-01000.csv data-00538-of-01000.csv data-00872-of-01000.csv\r\n",
      "data-00205-of-01000.csv data-00539-of-01000.csv data-00873-of-01000.csv\r\n",
      "data-00206-of-01000.csv data-00540-of-01000.csv data-00874-of-01000.csv\r\n",
      "data-00207-of-01000.csv data-00541-of-01000.csv data-00875-of-01000.csv\r\n",
      "data-00208-of-01000.csv data-00542-of-01000.csv data-00876-of-01000.csv\r\n",
      "data-00209-of-01000.csv data-00543-of-01000.csv data-00877-of-01000.csv\r\n",
      "data-00210-of-01000.csv data-00544-of-01000.csv data-00878-of-01000.csv\r\n",
      "data-00211-of-01000.csv data-00545-of-01000.csv data-00879-of-01000.csv\r\n",
      "data-00212-of-01000.csv data-00546-of-01000.csv data-00880-of-01000.csv\r\n",
      "data-00213-of-01000.csv data-00547-of-01000.csv data-00881-of-01000.csv\r\n",
      "data-00214-of-01000.csv data-00548-of-01000.csv data-00882-of-01000.csv\r\n",
      "data-00215-of-01000.csv data-00549-of-01000.csv data-00883-of-01000.csv\r\n",
      "data-00216-of-01000.csv data-00550-of-01000.csv data-00884-of-01000.csv\r\n",
      "data-00217-of-01000.csv data-00551-of-01000.csv data-00885-of-01000.csv\r\n",
      "data-00218-of-01000.csv data-00552-of-01000.csv data-00886-of-01000.csv\r\n",
      "data-00219-of-01000.csv data-00553-of-01000.csv data-00887-of-01000.csv\r\n",
      "data-00220-of-01000.csv data-00554-of-01000.csv data-00888-of-01000.csv\r\n",
      "data-00221-of-01000.csv data-00555-of-01000.csv data-00889-of-01000.csv\r\n",
      "data-00222-of-01000.csv data-00556-of-01000.csv data-00890-of-01000.csv\r\n",
      "data-00223-of-01000.csv data-00557-of-01000.csv data-00891-of-01000.csv\r\n",
      "data-00224-of-01000.csv data-00558-of-01000.csv data-00892-of-01000.csv\r\n",
      "data-00225-of-01000.csv data-00559-of-01000.csv data-00893-of-01000.csv\r\n",
      "data-00226-of-01000.csv data-00560-of-01000.csv data-00894-of-01000.csv\r\n",
      "data-00227-of-01000.csv data-00561-of-01000.csv data-00895-of-01000.csv\r\n",
      "data-00228-of-01000.csv data-00562-of-01000.csv data-00896-of-01000.csv\r\n",
      "data-00229-of-01000.csv data-00563-of-01000.csv data-00897-of-01000.csv\r\n",
      "data-00230-of-01000.csv data-00564-of-01000.csv data-00898-of-01000.csv\r\n",
      "data-00231-of-01000.csv data-00565-of-01000.csv data-00899-of-01000.csv\r\n",
      "data-00232-of-01000.csv data-00566-of-01000.csv data-00900-of-01000.csv\r\n",
      "data-00233-of-01000.csv data-00567-of-01000.csv data-00901-of-01000.csv\r\n",
      "data-00234-of-01000.csv data-00568-of-01000.csv data-00902-of-01000.csv\r\n",
      "data-00235-of-01000.csv data-00569-of-01000.csv data-00903-of-01000.csv\r\n",
      "data-00236-of-01000.csv data-00570-of-01000.csv data-00904-of-01000.csv\r\n",
      "data-00237-of-01000.csv data-00571-of-01000.csv data-00905-of-01000.csv\r\n",
      "data-00238-of-01000.csv data-00572-of-01000.csv data-00906-of-01000.csv\r\n",
      "data-00239-of-01000.csv data-00573-of-01000.csv data-00907-of-01000.csv\r\n",
      "data-00240-of-01000.csv data-00574-of-01000.csv data-00908-of-01000.csv\r\n",
      "data-00241-of-01000.csv data-00575-of-01000.csv data-00909-of-01000.csv\r\n",
      "data-00242-of-01000.csv data-00576-of-01000.csv data-00910-of-01000.csv\r\n",
      "data-00243-of-01000.csv data-00577-of-01000.csv data-00911-of-01000.csv\r\n",
      "data-00244-of-01000.csv data-00578-of-01000.csv data-00912-of-01000.csv\r\n",
      "data-00245-of-01000.csv data-00579-of-01000.csv data-00913-of-01000.csv\r\n",
      "data-00246-of-01000.csv data-00580-of-01000.csv data-00914-of-01000.csv\r\n",
      "data-00247-of-01000.csv data-00581-of-01000.csv data-00915-of-01000.csv\r\n",
      "data-00248-of-01000.csv data-00582-of-01000.csv data-00916-of-01000.csv\r\n",
      "data-00249-of-01000.csv data-00583-of-01000.csv data-00917-of-01000.csv\r\n",
      "data-00250-of-01000.csv data-00584-of-01000.csv data-00918-of-01000.csv\r\n",
      "data-00251-of-01000.csv data-00585-of-01000.csv data-00919-of-01000.csv\r\n",
      "data-00252-of-01000.csv data-00586-of-01000.csv data-00920-of-01000.csv\r\n",
      "data-00253-of-01000.csv data-00587-of-01000.csv data-00921-of-01000.csv\r\n",
      "data-00254-of-01000.csv data-00588-of-01000.csv data-00922-of-01000.csv\r\n",
      "data-00255-of-01000.csv data-00589-of-01000.csv data-00923-of-01000.csv\r\n",
      "data-00256-of-01000.csv data-00590-of-01000.csv data-00924-of-01000.csv\r\n",
      "data-00257-of-01000.csv data-00591-of-01000.csv data-00925-of-01000.csv\r\n",
      "data-00258-of-01000.csv data-00592-of-01000.csv data-00926-of-01000.csv\r\n",
      "data-00259-of-01000.csv data-00593-of-01000.csv data-00927-of-01000.csv\r\n",
      "data-00260-of-01000.csv data-00594-of-01000.csv data-00928-of-01000.csv\r\n",
      "data-00261-of-01000.csv data-00595-of-01000.csv data-00929-of-01000.csv\r\n",
      "data-00262-of-01000.csv data-00596-of-01000.csv data-00930-of-01000.csv\r\n",
      "data-00263-of-01000.csv data-00597-of-01000.csv data-00931-of-01000.csv\r\n",
      "data-00264-of-01000.csv data-00598-of-01000.csv data-00932-of-01000.csv\r\n",
      "data-00265-of-01000.csv data-00599-of-01000.csv data-00933-of-01000.csv\r\n",
      "data-00266-of-01000.csv data-00600-of-01000.csv data-00934-of-01000.csv\r\n",
      "data-00267-of-01000.csv data-00601-of-01000.csv data-00935-of-01000.csv\r\n",
      "data-00268-of-01000.csv data-00602-of-01000.csv data-00936-of-01000.csv\r\n",
      "data-00269-of-01000.csv data-00603-of-01000.csv data-00937-of-01000.csv\r\n",
      "data-00270-of-01000.csv data-00604-of-01000.csv data-00938-of-01000.csv\r\n",
      "data-00271-of-01000.csv data-00605-of-01000.csv data-00939-of-01000.csv\r\n",
      "data-00272-of-01000.csv data-00606-of-01000.csv data-00940-of-01000.csv\r\n",
      "data-00273-of-01000.csv data-00607-of-01000.csv data-00941-of-01000.csv\r\n",
      "data-00274-of-01000.csv data-00608-of-01000.csv data-00942-of-01000.csv\r\n",
      "data-00275-of-01000.csv data-00609-of-01000.csv data-00943-of-01000.csv\r\n",
      "data-00276-of-01000.csv data-00610-of-01000.csv data-00944-of-01000.csv\r\n",
      "data-00277-of-01000.csv data-00611-of-01000.csv data-00945-of-01000.csv\r\n",
      "data-00278-of-01000.csv data-00612-of-01000.csv data-00946-of-01000.csv\r\n",
      "data-00279-of-01000.csv data-00613-of-01000.csv data-00947-of-01000.csv\r\n",
      "data-00280-of-01000.csv data-00614-of-01000.csv data-00948-of-01000.csv\r\n",
      "data-00281-of-01000.csv data-00615-of-01000.csv data-00949-of-01000.csv\r\n",
      "data-00282-of-01000.csv data-00616-of-01000.csv data-00950-of-01000.csv\r\n",
      "data-00283-of-01000.csv data-00617-of-01000.csv data-00951-of-01000.csv\r\n",
      "data-00284-of-01000.csv data-00618-of-01000.csv data-00952-of-01000.csv\r\n",
      "data-00285-of-01000.csv data-00619-of-01000.csv data-00953-of-01000.csv\r\n",
      "data-00286-of-01000.csv data-00620-of-01000.csv data-00954-of-01000.csv\r\n",
      "data-00287-of-01000.csv data-00621-of-01000.csv data-00955-of-01000.csv\r\n",
      "data-00288-of-01000.csv data-00622-of-01000.csv data-00956-of-01000.csv\r\n",
      "data-00289-of-01000.csv data-00623-of-01000.csv data-00957-of-01000.csv\r\n",
      "data-00290-of-01000.csv data-00624-of-01000.csv data-00958-of-01000.csv\r\n",
      "data-00291-of-01000.csv data-00625-of-01000.csv data-00959-of-01000.csv\r\n",
      "data-00292-of-01000.csv data-00626-of-01000.csv data-00960-of-01000.csv\r\n",
      "data-00293-of-01000.csv data-00627-of-01000.csv data-00961-of-01000.csv\r\n",
      "data-00294-of-01000.csv data-00628-of-01000.csv data-00962-of-01000.csv\r\n",
      "data-00295-of-01000.csv data-00629-of-01000.csv data-00963-of-01000.csv\r\n",
      "data-00296-of-01000.csv data-00630-of-01000.csv data-00964-of-01000.csv\r\n",
      "data-00297-of-01000.csv data-00631-of-01000.csv data-00965-of-01000.csv\r\n",
      "data-00298-of-01000.csv data-00632-of-01000.csv data-00966-of-01000.csv\r\n",
      "data-00299-of-01000.csv data-00633-of-01000.csv data-00967-of-01000.csv\r\n",
      "data-00300-of-01000.csv data-00634-of-01000.csv data-00968-of-01000.csv\r\n",
      "data-00301-of-01000.csv data-00635-of-01000.csv data-00969-of-01000.csv\r\n",
      "data-00302-of-01000.csv data-00636-of-01000.csv data-00970-of-01000.csv\r\n",
      "data-00303-of-01000.csv data-00637-of-01000.csv data-00971-of-01000.csv\r\n",
      "data-00304-of-01000.csv data-00638-of-01000.csv data-00972-of-01000.csv\r\n",
      "data-00305-of-01000.csv data-00639-of-01000.csv data-00973-of-01000.csv\r\n",
      "data-00306-of-01000.csv data-00640-of-01000.csv data-00974-of-01000.csv\r\n",
      "data-00307-of-01000.csv data-00641-of-01000.csv data-00975-of-01000.csv\r\n",
      "data-00308-of-01000.csv data-00642-of-01000.csv data-00976-of-01000.csv\r\n",
      "data-00309-of-01000.csv data-00643-of-01000.csv data-00977-of-01000.csv\r\n",
      "data-00310-of-01000.csv data-00644-of-01000.csv data-00978-of-01000.csv\r\n",
      "data-00311-of-01000.csv data-00645-of-01000.csv data-00979-of-01000.csv\r\n",
      "data-00312-of-01000.csv data-00646-of-01000.csv data-00980-of-01000.csv\r\n",
      "data-00313-of-01000.csv data-00647-of-01000.csv data-00981-of-01000.csv\r\n",
      "data-00314-of-01000.csv data-00648-of-01000.csv data-00982-of-01000.csv\r\n",
      "data-00315-of-01000.csv data-00649-of-01000.csv data-00983-of-01000.csv\r\n",
      "data-00316-of-01000.csv data-00650-of-01000.csv data-00984-of-01000.csv\r\n",
      "data-00317-of-01000.csv data-00651-of-01000.csv data-00985-of-01000.csv\r\n",
      "data-00318-of-01000.csv data-00652-of-01000.csv data-00986-of-01000.csv\r\n",
      "data-00319-of-01000.csv data-00653-of-01000.csv data-00987-of-01000.csv\r\n",
      "data-00320-of-01000.csv data-00654-of-01000.csv data-00988-of-01000.csv\r\n",
      "data-00321-of-01000.csv data-00655-of-01000.csv data-00989-of-01000.csv\r\n",
      "data-00322-of-01000.csv data-00656-of-01000.csv data-00990-of-01000.csv\r\n",
      "data-00323-of-01000.csv data-00657-of-01000.csv data-00991-of-01000.csv\r\n",
      "data-00324-of-01000.csv data-00658-of-01000.csv data-00992-of-01000.csv\r\n",
      "data-00325-of-01000.csv data-00659-of-01000.csv data-00993-of-01000.csv\r\n",
      "data-00326-of-01000.csv data-00660-of-01000.csv data-00994-of-01000.csv\r\n",
      "data-00327-of-01000.csv data-00661-of-01000.csv data-00995-of-01000.csv\r\n",
      "data-00328-of-01000.csv data-00662-of-01000.csv data-00996-of-01000.csv\r\n",
      "data-00329-of-01000.csv data-00663-of-01000.csv data-00997-of-01000.csv\r\n",
      "data-00330-of-01000.csv data-00664-of-01000.csv data-00998-of-01000.csv\r\n",
      "data-00331-of-01000.csv data-00665-of-01000.csv data-00999-of-01000.csv\r\n",
      "data-00332-of-01000.csv data-00666-of-01000.csv\r\n",
      "data-00333-of-01000.csv data-00667-of-01000.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls {DATA_DIR}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [OPTIONAL] Run data extraction pipeline using Cloud Dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'ksalama-cloudml'\n",
    "REGION = 'europe-west1'\n",
    "WORKSPACE = 'gs://ksalama-cloudml/workspace/playlist2ann'\n",
    "DATA_DIR = '{}/data'.format(WORKSPACE)\n",
    "COOC_DIR = '{}/cooc'.format(WORKSPACE)\n",
    "STAGING_DIR ='{}/stg'.format(WORKSPACE)\n",
    "TEMP_DIR ='{}/tmp'.format(WORKSPACE)\n",
    "\n",
    "runner = 'DataflowRunner'\n",
    "job_name = 'playlist-data-extraction-{}'.format(datetime.utcnow().strftime('%y%m%d-%H%M%S'))\n",
    "\n",
    "source_query = generate_query(min_occurrence=50)\n",
    "\n",
    "args = {\n",
    "    'job_name': job_name,\n",
    "    'runner': runner,\n",
    "    'source_query': source_query,\n",
    "    'sink_data_location': DATA_DIR,\n",
    "    'project': PROJECT_ID,\n",
    "    'region': REGION,\n",
    "    'staging_location': STAGING_DIR,\n",
    "    'temp_location': TEMP_DIR,\n",
    "    'save_main_session': True,\n",
    "}\n",
    "print(\"Pipeline args are set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running data extraction pipeline...\")\n",
    "run_extraction_pipeline(args)\n",
    "print(\"Pipeline is done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls gs://ksalama-cloudml/workspace/playlist2ann/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute co-occurrence statistics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(pipeline, source_data_location):\n",
    "    raw_data = ( \n",
    "        pipeline\n",
    "        | 'Read from CSV'>> beam.io.ReadFromText(\n",
    "            file_pattern=source_data_location)\n",
    "    )\n",
    "    return raw_data\n",
    "    \n",
    "\n",
    "def parse_data(raw_data):\n",
    "    \n",
    "    def _parse_csv(line):\n",
    "        try:\n",
    "            context_id, item_id = line.split(',')\n",
    "            return (context_id, item_id)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid file format. A comma-separated data with two values is expected.\")\n",
    "            \n",
    "    parsed_data = (\n",
    "        raw_data\n",
    "        | 'Parse to tuple' >> beam.Map(_parse_csv)\n",
    "    \n",
    "    )\n",
    "    return parsed_data\n",
    "    \n",
    "\n",
    "def compute_item_frequency(raw_data, min_freq):\n",
    "    \n",
    "    def _make_item_as_key(record):\n",
    "        context_id, item_id = record\n",
    "        return (item_id, context_id)\n",
    "    \n",
    "    def _include(record):\n",
    "        item, count = record\n",
    "        return count >= min_freq\n",
    "    \n",
    "    item_frequency = (\n",
    "        raw_data\n",
    "        | 'Make item as key'>> beam.Map(_make_item_as_key)\n",
    "        | 'Count item frequency' >> beam.CombinePerKey(beam.combiners.CountCombineFn())\n",
    "        | 'Filter infrequent items' >> beam.Filter(_include)\n",
    "    )\n",
    "    return item_frequency\n",
    "\n",
    "def compute_cooccurrence(parsed_data):\n",
    "    \n",
    "    def _has_multiple_items(record):\n",
    "        _, items = record\n",
    "        return len(list(items)) > 1\n",
    "\n",
    "    def _create_item_pairs(record):\n",
    "        _, items = record\n",
    "        items = list(items)\n",
    "        result = list()\n",
    "        count = len(items)\n",
    "        for i in range(count):\n",
    "            for j in range(i+1, count):\n",
    "                first, second = (items[i], items[j]) if  items[i] < items[j] else (items[j], items[i])\n",
    "                result.append(\n",
    "                    ((first, second), 1))\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _format_output(record):\n",
    "        key, value = record\n",
    "        item_1, item_2 = key\n",
    "        return (item_1, item_2, value)\n",
    "    \n",
    "    cooccurrence = (\n",
    "        parsed_data\n",
    "        | 'Group by context id' >> beam.GroupByKey()\n",
    "        | 'Filter groups with single item' >> beam.Filter(_has_multiple_items)\n",
    "        | 'Create item pairs' >> beam.FlatMap(_create_item_pairs)\n",
    "        | 'Compute cooccurrence' >> beam.CombinePerKey(sum)\n",
    "        | 'Format cooc' >> beam.Map(_format_output)\n",
    "        \n",
    "    )\n",
    "    return cooccurrence\n",
    "\n",
    "def compute_total_cooc(cooccurrence):\n",
    "    \n",
    "    def _get_cooc_values(record):\n",
    "        _, _, cooc = record\n",
    "        return cooc\n",
    "    \n",
    "    result = (\n",
    "        cooccurrence\n",
    "        | \"Get cooc values\" >> beam.Map(_get_cooc_values)\n",
    "        | \"Sum cooc values\" >> beam.CombineGlobally(sum)\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "def join_with_item_frequency(cooccurrence, item_frequency):\n",
    "    \n",
    "    def _make_item1_key(record):\n",
    "        item1, item2, cooc = record\n",
    "        return (item1, (item2, cooc))\n",
    "        \n",
    "    def _make_item2_key(record):\n",
    "        item1, item2, cooc, freq1 = record\n",
    "        return (item2, (item1, cooc, freq1))\n",
    "    \n",
    "    def _is_frequent_item(record):\n",
    "        _, entry = record\n",
    "        return len(list(entry['freq'])) == 1\n",
    "    \n",
    "    def _reformat1(record):\n",
    "        result = []\n",
    "        item1, entry = record\n",
    "        cooc_items = entry['cooc']\n",
    "        item1_freq = entry['freq']\n",
    "        \n",
    "        for item2, cooc in cooc_items:\n",
    "            result.append(\n",
    "                (item1, item2, cooc, item1_freq[0])\n",
    "            )\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def _reformat2(record):\n",
    "        result = []\n",
    "        item2, entry = record\n",
    "        cooc_items = entry['cooc']\n",
    "        item2_freq = entry['freq']\n",
    "        \n",
    "        for item1, cooc, freq1 in cooc_items:\n",
    "            result.append(\n",
    "                (item1, item2, cooc, freq1, item2_freq[0])\n",
    "            )\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    item1_cooc = (\n",
    "        cooccurrence\n",
    "        | \"Make item 1 as key\" >> beam.Map(_make_item1_key)\n",
    "    )\n",
    "    \n",
    "    cooc_and_freq1 = (\n",
    "        {'cooc': item1_cooc, 'freq': item_frequency}\n",
    "        | \"Join with item 1 frequency\" >> beam.CoGroupByKey()\n",
    "        | \"Filter frequent items 1\" >> beam.Filter(_is_frequent_item)\n",
    "        | \"Reformat\" >> beam.FlatMap(_reformat1)\n",
    "        \n",
    "    )\n",
    "    \n",
    "    item2_cooc = (\n",
    "        cooc_and_freq1\n",
    "        | \"Make item 2 as key\" >> beam.Map(_make_item2_key)\n",
    "    )\n",
    "    \n",
    "    stats = (\n",
    "        {'cooc': item2_cooc, 'freq': item_frequency}\n",
    "        | \"Join with item 2 frequency\" >> beam.CoGroupByKey()\n",
    "        | \"Filter frequent items 2\" >> beam.Filter(_is_frequent_item)\n",
    "        | \"Reformat again\" >> beam.FlatMap(_reformat2)\n",
    "    )\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "def vocabulary(item_frequency):\n",
    "    \n",
    "    def _get_vocab(record):\n",
    "        item, _ = record\n",
    "        return item\n",
    "    \n",
    "    vocab = (\n",
    "        item_frequency\n",
    "        | \"Extract item vocabulary\" >> beam.Map(_get_vocab)\n",
    "\n",
    "    )\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "def create_partitions(vocab, num_shards):\n",
    "    \n",
    "    def _partition_fn(item, num_shards):\n",
    "        return abs(hash(item)) % num_shards\n",
    "    \n",
    "    partitions = (\n",
    "        vocab \n",
    "        | \"Parition items\" >> beam.Partition(_partition_fn, num_shards)\n",
    "    )\n",
    "    \n",
    "    return partitions\n",
    "\n",
    "    \n",
    "def create_top_pairs(index, item_frequency, top_count):\n",
    "    \n",
    "    def _compare(item_freq1, item_freq2):\n",
    "        _, freq1 = item_freq1\n",
    "        _, freq2 = item_freq2\n",
    "        return freq1 < freq2\n",
    "    \n",
    "    def _generate_pairs(items):\n",
    "        results = []\n",
    "        items = list(items)\n",
    "        count = len(items)\n",
    "        for i in range(count):\n",
    "            for j in range(i+1, count):\n",
    "                item1 = items[i][0]\n",
    "                item2 = items[j][0]\n",
    "                first, second = (item1, item2) if  item1 < item2 else (item2, item1)\n",
    "                results.append((first, second, 0))\n",
    "        return results\n",
    "    \n",
    "    pairs = (\n",
    "        item_frequency\n",
    "        | \"Get top items - shard {}\".format(index) >> beam.CombineGlobally(beam.combiners.TopCombineFn(top_count, _compare))\n",
    "        | \"Generate item pairs - shard {}\".format(index) >> beam.FlatMap(_generate_pairs)\n",
    "    )\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "def generate_and_union_pairs(partitions, top_count):\n",
    "    \n",
    "    pair_list = []\n",
    "    for i, partition in enumerate(partitions):\n",
    "        pairs = create_top_pairs(i+1, partition, top_count)\n",
    "        pair_list.append(pairs)\n",
    "            \n",
    "    pairs = (\n",
    "        pair_list\n",
    "        | \"Union item pairs\" >> beam.Flatten()\n",
    "    )\n",
    "    return pairs\n",
    "    \n",
    "def merge_pairs(pairs):\n",
    "    \n",
    "    def _pair_as_key(record):\n",
    "        item1, item2, cooc = record\n",
    "        return ((item1, item2), cooc)\n",
    "    \n",
    "    def _process_pair(record):\n",
    "        key, value = record\n",
    "        item1, item2 = key\n",
    "        cooc = max(value)\n",
    "        return (item1, item2, cooc)\n",
    "    \n",
    "    pairs = (\n",
    "        pairs\n",
    "        | \"Merge item pairs\" >> beam.Flatten()\n",
    "        | \"Use pair as key\" >> beam.Map(_pair_as_key)\n",
    "        | \"Group pairs by key\" >> beam.GroupByKey()\n",
    "        | \"Process pairs\" >> beam.Map(_process_pair)\n",
    "        \n",
    "    )\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def compute_score(data, total):\n",
    "    \n",
    "    def _compute_pmi(record, total):\n",
    "        import math\n",
    "        item1, item2, cooc, freq1, freq2 = record\n",
    "        \n",
    "        if cooc > 0:\n",
    "            pmi = math.log(cooc) - math.log(freq1) - math.log(freq2) + math.log(total)\n",
    "            weight = math.sqrt(cooc)\n",
    "            sample_type = 'P'\n",
    "        else:\n",
    "            pmi = math.log(1) - math.log(freq1) - math.log(freq2) + math.log(total)\n",
    "            weight = 1\n",
    "            sample_type = 'N'\n",
    "        return (item1, item2, round(pmi, 5), round(weight, 5), sample_type)\n",
    "    \n",
    "    \n",
    "    stats = (\n",
    "        data\n",
    "        | \"Compute pairewise mutual infromation\" >> beam.Map(_compute_pmi, beam.pvalue.AsSingleton(total))\n",
    "    )\n",
    "    return stats\n",
    "\n",
    "def get_info(stats):\n",
    "    \n",
    "    def _make_type_as_key(record):\n",
    "        _, _, _, _, record_type = record\n",
    "        return (record_type, 1)\n",
    "    \n",
    "    def _get_scores(record):\n",
    "        _, _, score, _, _ = record\n",
    "        return score\n",
    "    \n",
    "    counts = (\n",
    "        stats\n",
    "        | \"Group by record type\" >> beam.Map(_make_type_as_key)\n",
    "        | \"Count records\" >> beam.CombinePerKey(sum)\n",
    "        | \"Fromat counts\" >> beam.Map(lambda entry: '{}: {}'.format(entry[0], entry[1]))\n",
    "    )\n",
    "    \n",
    "    scores = (\n",
    "        stats\n",
    "        | \"Get scores\" >> beam.Map(_get_scores)\n",
    "    )\n",
    "    \n",
    "    mins = (\n",
    "        scores\n",
    "        | \"Get min score\" >> beam.CombineGlobally(min).without_defaults()\n",
    "        | \"Format min score\" >> beam.Map(lambda value: 'min: {}'.format(value))\n",
    "    )\n",
    "    \n",
    "    maxs = (\n",
    "        scores\n",
    "        | \"Get max score\" >> beam.CombineGlobally(max).without_defaults()\n",
    "        | \"Format max score\" >> beam.Map(lambda value: 'max: {}'.format(value))\n",
    "    )\n",
    "    \n",
    "    info = (\n",
    "        (counts, mins, maxs)\n",
    "        | \"Combine info\" >> beam.Flatten()\n",
    "    )\n",
    "    \n",
    "    return info\n",
    "    \n",
    "\n",
    "def write_debug(data, sink_data_location):\n",
    "    \n",
    "    (\n",
    "        data\n",
    "        | 'Write debug' >> beam.io.WriteToText(\n",
    "            file_path_prefix = sink_data_location+\"/debug\")\n",
    "    )\n",
    "    \n",
    "\n",
    "def write_log(info, sink_data_location):\n",
    "    \n",
    "    (\n",
    "        info\n",
    "        | 'Write logs' >> beam.io.WriteToText(\n",
    "            file_path_prefix = sink_data_location+\"/info\",\n",
    "            file_name_suffix = \".log\",\n",
    "            shard_name_template ='',\n",
    "            num_shards = 1)\n",
    "    )\n",
    "\n",
    "def write_vocab(vocab, sink_data_location):\n",
    "    \n",
    "    (\n",
    "        vocab\n",
    "        | 'Write vocabulary file' >> beam.io.WriteToText(\n",
    "            file_path_prefix = sink_data_location+\"/vocab\", \n",
    "            file_name_suffix = \".txt\",\n",
    "            shard_name_template ='',\n",
    "            num_shards = 1)\n",
    "    )\n",
    "    \n",
    "\n",
    "def write_to_tfrecords(stats, sink_data_location):\n",
    "    \n",
    "    def _to_tf_example(record):\n",
    "        item1, item2, score, weight, record_type = record\n",
    "        \n",
    "        entry1 = {\n",
    "            'item1': tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(value=[tf.compat.as_bytes(item1)])),\n",
    "            'item2': tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(value=[tf.compat.as_bytes(item2)])),\n",
    "            'score': tf.train.Feature(\n",
    "                float_list=tf.train.FloatList(value=[float(score)])),\n",
    "            'weight': tf.train.Feature(\n",
    "                float_list=tf.train.FloatList(value=[float(weight)])),\n",
    "            'type': tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(value=[tf.compat.as_bytes(record_type)])),\n",
    "        }\n",
    "        \n",
    "        entry2 = {\n",
    "            'item1': tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(value=[tf.compat.as_bytes(item2)])),\n",
    "            'item2': tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(value=[tf.compat.as_bytes(item1)])),\n",
    "            'score': tf.train.Feature(\n",
    "                float_list=tf.train.FloatList(value=[float(score)])),\n",
    "            'weight': tf.train.Feature(\n",
    "                float_list=tf.train.FloatList(value=[float(weight)])),\n",
    "            'type': tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(value=[tf.compat.as_bytes(record_type)])),\n",
    "        }\n",
    "        return [\n",
    "            tf.train.Example(features=tf.train.Features(feature=entry1)),\n",
    "            tf.train.Example(features=tf.train.Features(feature=entry2)),\n",
    "        ]\n",
    "        \n",
    "    (\n",
    "        stats\n",
    "        | 'Encode to tf.example' >> beam.FlatMap(_to_tf_example)\n",
    "        | 'Serialize to string' >> beam.Map(lambda example: example.SerializeToString(deterministic=True))\n",
    "        | 'Shuffle data' >> beam.Reshuffle()\n",
    "        | 'Write to TFRecords files' >> beam.io.WriteToTFRecord(\n",
    "                file_path_prefix = sink_data_location+\"/cooc\",\n",
    "                file_name_suffix = '.tfrecords')\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-occurence pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cooc_pipeline(args):\n",
    "\n",
    "    source_data_location = args['source_data_location']\n",
    "    sink_data_location = args['sink_data_location']\n",
    "    runner = args['runner']\n",
    "    min_freq = args['min_freq']\n",
    "    num_shards = args['num_shards']\n",
    "    top_count = args['top_count']\n",
    "    \n",
    "    pipeline_options = beam.options.pipeline_options.GoogleCloudOptions(**args)\n",
    "    \n",
    "    with beam.Pipeline(runner, options=pipeline_options) as pipeline:\n",
    "        # Read data from source files\n",
    "        raw_data = read_data(pipeline, source_data_location)\n",
    "        \n",
    "        # Parse data to (context_id, item_id)\n",
    "        parsed_data = parse_data(raw_data)\n",
    "        \n",
    "        # Compute frequency of each item (item_id, frequency)\n",
    "        item_frequency = compute_item_frequency(parsed_data, min_freq)\n",
    "        #write_debug(item_frequency, sink_data_location)\n",
    "        \n",
    "        # Extract distinct list of items (vocabulary)\n",
    "        vocab = vocabulary(item_frequency)\n",
    "        write_vocab(vocab, sink_data_location)\n",
    "        \n",
    "        # Generate pairs (item_1, item_2, cooc) for the top frequent items. cooc is set to 0.\n",
    "        top_pairs = create_top_pairs(0, item_frequency, top_count)\n",
    "        #write_debug(top_pairs, sink_data_location)\n",
    "        \n",
    "        # Split items to partitions\n",
    "        partitions = create_partitions(item_frequency, num_shards)\n",
    "        #write_debug(partitions[0], sink_data_location)\n",
    "        \n",
    "        # For each partition, generate  (item_1, item_2, score) for the top frequent items. Then union.\n",
    "        top_pairs_per_partition = generate_and_union_pairs(partitions, top_count)\n",
    "        #write_debug(pairs, sink_data_location)\n",
    "        \n",
    "        #For the co-occuring items, compute the cooccurrence (item_1, item_2, cooc)\n",
    "        cooccurrence = compute_cooccurrence(parsed_data)\n",
    "        #write_debug(cooccurrence, sink_data_location)\n",
    "        \n",
    "        # Merge all pairs: the co-occuring (positive) and not co-occuring (negative)\n",
    "        all_pairs = merge_pairs((cooccurrence, top_pairs, top_pairs_per_partition))\n",
    "        #write_debug(all_pairs, sink_data_location)\n",
    "        \n",
    "        # Compute |D| = \\sum_{ij} x_{ij} \n",
    "        total = compute_total_cooc(cooccurrence)\n",
    "        #write_debug(total, sink_data_location)\n",
    "        \n",
    "        # Join Cooc with item frequency => (item_1, item_2, cooc, freq_1, freq_2)\n",
    "        join = join_with_item_frequency(all_pairs, item_frequency)\n",
    "        #write_debug(join, sink_data_location)\n",
    "        \n",
    "        # Compute statistics (pmi), weights, and record type => (item_1, item_2, pmi, weight, type)\n",
    "        stats = compute_score(join, total)\n",
    "        #write_debug(stats, sink_data_location)\n",
    "        write_to_tfrecords(stats, sink_data_location)\n",
    "        \n",
    "        # Log information about the created dataset\n",
    "        info = get_info(stats)\n",
    "        write_log(info, sink_data_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline args are set.\n"
     ]
    }
   ],
   "source": [
    "runner = 'DirectRunner'\n",
    "job_name = 'test-cooc-{}'.format(datetime.utcnow().strftime('%y%m%d-%H%M%S'))\n",
    "\n",
    "args = {\n",
    "    'job_name': job_name,\n",
    "    'runner': runner,\n",
    "    'source_data_location': '{}/data-*.csv'.format(DATA_DIR),\n",
    "    'sink_data_location': COOC_DIR,\n",
    "    'min_freq': 5,\n",
    "    'top_count': 100,\n",
    "    'num_shards': 100,\n",
    "    'project': PROJECT_ID,\n",
    "}\n",
    "print(\"Pipeline args are set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Co-occurrence pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cooc pipeline...\n"
     ]
    }
   ],
   "source": [
    "time_start = datetime.utcnow() \n",
    "print(\"Running cooc pipeline...\")\n",
    "run_cooc_pipeline(args)\n",
    "print(\"Pipeline is done.\")\n",
    "time_end = datetime.utcnow() \n",
    "time_elapsed = time_end - time_start\n",
    "print(\"Execution elapsed time: {} seconds\".format(time_elapsed.total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cooc-00000-of-00001.tfrecords info2.log\r\n",
      "info.log                      vocab.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls {COOC_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: 2249477\r\n",
      "N: 486352\r\n",
      "max: 13.33242\r\n",
      "min: 2.2812\r\n"
     ]
    }
   ],
   "source": [
    "!head {COOC_DIR}/info.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Read TFRecords using tf.data APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_fn(file_pattern, batch_size):\n",
    "    \n",
    "    features = {\n",
    "        'item1': tf.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'item2': tf.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'score': tf.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "        'weight': tf.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "        'type': tf.FixedLenFeature(dtype=tf.string, shape=())\n",
    "    }\n",
    "\n",
    "    def _input_fn():\n",
    "        dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "            file_pattern,\n",
    "            batch_size,\n",
    "            features,\n",
    "            reader=tf.data.TFRecordDataset,\n",
    "            label_key=None,\n",
    "            num_epochs=1,\n",
    "            shuffle=True\n",
    "        )\n",
    "        return dataset\n",
    "    \n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/khalidsalama/Technology/GoogleCloud/GCP-Github/kfp-components/google/tf_hub/tabular2cooc/venv/lib/python3.6/site-packages/tensorflow/python/data/experimental/ops/readers.py:835: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
      "\n",
      "Record 1:\n",
      "-item1:[b'259762' b'324089' b'263416' b'3130567' b'2961260']\n",
      "-item2:[b'991115' b'4299822' b'3156107' b'699438' b'3114309']\n",
      "-score:[ 9.36591 10.08983  9.66374  7.32508  8.36261]\n",
      "-type:[b'N' b'P' b'P' b'P' b'N']\n",
      "-weight:[1. 1. 1. 1. 1.]\n",
      "\n",
      "Record 2:\n",
      "-item1:[b'1155014' b'558543' b'3871848' b'1121660' b'1104682']\n",
      "-item2:[b'3140490' b'594827' b'4190858' b'759854' b'1184321']\n",
      "-score:[ 6.95162  9.86668 11.32197 11.65844  9.64354]\n",
      "-type:[b'P' b'P' b'P' b'P' b'P']\n",
      "-weight:[1. 1. 1. 1. 1.]\n",
      "\n",
      "Record 3:\n",
      "-item1:[b'1104624' b'3788311' b'2330675' b'1100002' b'660399']\n",
      "-item2:[b'2829121' b'881477' b'2714414' b'559941' b'795275']\n",
      "-score:[10.15437  8.32624  6.66444  5.53245  9.93122]\n",
      "-type:[b'P' b'P' b'P' b'P' b'P']\n",
      "-weight:[1. 1. 1. 1. 1.]\n",
      "\n",
      "Record 4:\n",
      "-item1:[b'2149180' b'15347259' b'365260' b'2114662' b'1115046']\n",
      "-item2:[b'2235787' b'4417' b'916362' b'40494' b'1154094']\n",
      "-score:[ 8.54827  8.68547  8.62831 10.91651  7.96209]\n",
      "-type:[b'N' b'P' b'N' b'P' b'N']\n",
      "-weight:[1. 1. 1. 1. 1.]\n",
      "\n",
      "Record 5:\n",
      "-item1:[b'1169860' b'1049319' b'3976604' b'1101480' b'3157970']\n",
      "-item2:[b'1181249' b'1075781' b'577' b'1143621' b'4192138']\n",
      "-score:[11.69666  6.50979 10.91651  8.10309 11.79197]\n",
      "-type:[b'P' b'P' b'P' b'N' b'P']\n",
      "-weight:[1.41421 1.      1.      1.      1.     ]\n"
     ]
    }
   ],
   "source": [
    "tf.enable_eager_execution()\n",
    "\n",
    "DATA_FILES = \"{}/cooc-*\".format(COOC_DIR)\n",
    "\n",
    "dataset = make_input_fn(DATA_FILES, batch_size=5)()\n",
    "for i, features in enumerate(dataset.take(5)):\n",
    "    print()\n",
    "    print(\"Record {}:\".format(i+1))\n",
    "    for key in features:\n",
    "        print(\"-{}:{}\".format(key, features[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
