{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Co-occurence Statistics from Text\n",
    "\n",
    "This tutorial shows how to compute **Co-occurrence statistics** for words in a given text corups.\n",
    "\n",
    "The statistics we compute for each co-occurring pair is the [Pointwise mutual information](https://en.wikipedia.org/wiki/Pointwise_mutual_information) (pmi), which is used by the [Swivel](https://arxiv.org/pdf/1602.02215.pdf) algorithm for learning embeddings.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/ksalama/data2cooc2emb2ann/blob/master/text2emb/01-Creating_Co-occurence_Statistics_from_Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import apache_beam as beam\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'ksalama-cloudml'\n",
    "REGION = 'europe-west1'\n",
    "WORKSPACE = './workspace'\n",
    "DATA_DIR = '{}/data'.format(WORKSPACE)\n",
    "COOC_DIR = '{}/cooc'.format(WORKSPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.io.gfile.exists(COOC_DIR):\n",
    "    print(\"Removing {} contents...\".format(COOC_DIR))\n",
    "    tf.io.gfile.rmtree(COOC_DIR)\n",
    "\n",
    "print(\"Creating output: {}\".format(COOC_DIR))\n",
    "tf.io.gfile.makedirs(COOC_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \n",
    "             \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \n",
    "             \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \n",
    "             \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \n",
    "             \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \n",
    "             \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \n",
    "             \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \n",
    "             \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \n",
    "             \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \n",
    "             \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \n",
    "             \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \n",
    "             \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \n",
    "             \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \n",
    "             \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(pipeline, source_data_location):\n",
    "    lines = ( \n",
    "        pipeline\n",
    "        | 'Read from files'>> beam.io.ReadFromText(\n",
    "            file_pattern=source_data_location)\n",
    "        | 'Skip empty lines'>> beam.Filter(lambda line: len(line) > 2)\n",
    "    )\n",
    "    return lines\n",
    "\n",
    "\n",
    "def tokenize(lines, min_sentence_length, stop_words):\n",
    "\n",
    "    import string\n",
    "    \n",
    "    def _tokenize(line, stop_words):\n",
    "        line = ''.join(ch for ch in line if ch not in set(string.punctuation)).strip().lower()\n",
    "        tokens = [token for token in line.split() if len(token) > 2 and not token.isdigit()]\n",
    "        for word in list(stop_words):\n",
    "            if word in tokens:\n",
    "                tokens.remove(word)\n",
    "        return tokens\n",
    "    \n",
    "    def _valid(tokens, min_sentence_length):\n",
    "        return len(list(tokens)) >= min_sentence_length\n",
    "\n",
    "    \n",
    "    line_words = ( \n",
    "        lines\n",
    "        | 'Tokenize lines'>> beam.Map(_tokenize, stop_words)\n",
    "        | 'Filter invalid lines'>> beam.Filter(_valid, min_sentence_length)\n",
    "    )\n",
    "    return line_words\n",
    "\n",
    "\n",
    "def compute_word_frequency(line_words, min_freq):\n",
    "    \n",
    "    def _count_words(line_words):\n",
    "        from collections import Counter\n",
    "        word_counts = list(Counter(line_words).items())\n",
    "        return word_counts\n",
    "    \n",
    "    def _include(word_counts):\n",
    "        word, count = word_counts\n",
    "        return count >= min_freq\n",
    "    \n",
    "    word_counts = (\n",
    "        line_words\n",
    "        | 'Word count per line'>> beam.FlatMap(_count_words)\n",
    "        | 'Compute word frequency' >> beam.CombinePerKey(sum)\n",
    "        | 'Filter infrequent words' >> beam.Filter(_include)\n",
    "    )\n",
    "    return word_counts\n",
    "\n",
    "\n",
    "def compute_cooccurrence(line_words, window_size):\n",
    "    \n",
    "    from collections import defaultdict\n",
    "    \n",
    "    def _extract_coocs_from_line(line_words, window_size):\n",
    "\n",
    "        coocs = defaultdict(float)\n",
    "        \n",
    "        for position, word1 in enumerate(line_words):\n",
    "            window_extent = min(window_size + 1, len(line_words) - position)\n",
    "            for offset in range(1, window_extent):\n",
    "                word2 = line_words[position + offset]\n",
    "                if word1 != word2:\n",
    "                    pair = (min(word1, word2), max(word1, word2))\n",
    "                    count = 1.0 / offset\n",
    "                    coocs[pair] += count\n",
    "        \n",
    "        return list(coocs.items())\n",
    "\n",
    "    def _format_output(record):\n",
    "        key, value = record\n",
    "        item_1, item_2 = key\n",
    "        return (item_1, item_2, value)\n",
    "    \n",
    "    cooccurrence = (\n",
    "        line_words\n",
    "        | 'Extract cooccurrence from line' >> beam.FlatMap(_extract_coocs_from_line, window_size)\n",
    "        | 'Aggregate cooccurrences' >> beam.CombinePerKey(sum)\n",
    "        | 'Format cooc' >> beam.Map(_format_output)\n",
    "        \n",
    "    )\n",
    "    return cooccurrence\n",
    "\n",
    "def compute_total_cooc(cooccurrence):\n",
    "    \n",
    "    def _get_cooc_values(record):\n",
    "        _, _, cooc = record\n",
    "        return cooc\n",
    "    \n",
    "    result = (\n",
    "        cooccurrence\n",
    "        | \"Get cooc values\" >> beam.Map(_get_cooc_values)\n",
    "        | \"Sum cooc values\" >> beam.CombineGlobally(sum)\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "def join_with_item_frequency(cooccurrence, item_frequency):\n",
    "    \n",
    "    def _make_item1_key(record):\n",
    "        item1, item2, cooc = record\n",
    "        return (item1, (item2, cooc))\n",
    "        \n",
    "    def _make_item2_key(record):\n",
    "        item1, item2, cooc, freq1 = record\n",
    "        return (item2, (item1, cooc, freq1))\n",
    "    \n",
    "    def _is_frequent_item(record):\n",
    "        _, entry = record\n",
    "        return len(list(entry['freq'])) == 1\n",
    "    \n",
    "    def _reformat1(record):\n",
    "        result = []\n",
    "        item1, entry = record\n",
    "        cooc_items = entry['cooc']\n",
    "        item1_freq = entry['freq']\n",
    "        \n",
    "        for item2, cooc in cooc_items:\n",
    "            result.append(\n",
    "                (item1, item2, cooc, item1_freq[0]))\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def _reformat2(record):\n",
    "        result = []\n",
    "        item2, entry = record\n",
    "        cooc_items = entry['cooc']\n",
    "        item2_freq = entry['freq']\n",
    "        \n",
    "        for item1, cooc, freq1 in cooc_items:\n",
    "            result.append(\n",
    "                (item1, item2, cooc, freq1, item2_freq[0]))\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    item1_cooc = (\n",
    "        cooccurrence\n",
    "        | \"Make item 1 as key\" >> beam.Map(_make_item1_key)\n",
    "    )\n",
    "    \n",
    "    cooc_and_freq1 = (\n",
    "        {'cooc': item1_cooc, 'freq': item_frequency}\n",
    "        | \"Join with item 1 frequency\" >> beam.CoGroupByKey()\n",
    "        | \"Filter frequent items 1\" >> beam.Filter(_is_frequent_item)\n",
    "        | \"Format\" >> beam.FlatMap(_reformat1)\n",
    "        \n",
    "    )\n",
    "    \n",
    "    item2_cooc = (\n",
    "        cooc_and_freq1\n",
    "        | \"Make item 2 as key\" >> beam.Map(_make_item2_key)\n",
    "    )\n",
    "    \n",
    "    stats = (\n",
    "        {'cooc': item2_cooc, 'freq': item_frequency}\n",
    "        | \"Join with item 2 frequency\" >> beam.CoGroupByKey()\n",
    "        | \"Filter frequent items 2\" >> beam.Filter(_is_frequent_item)\n",
    "        | \"Reformat\" >> beam.FlatMap(_reformat2)\n",
    "    )\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "def vocabulary(item_frequency):\n",
    "    \n",
    "    def _get_vocab(record):\n",
    "        item, _ = record\n",
    "        return item\n",
    "    \n",
    "    vocab = (\n",
    "        item_frequency\n",
    "        | \"Extract item vocabulary\" >> beam.Map(_get_vocab)\n",
    "\n",
    "    )\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "def create_partitions(vocab, num_shards):\n",
    "    \n",
    "    def _partition_fn(item, num_shards):\n",
    "        return abs(hash(item)) % num_shards\n",
    "    \n",
    "    partitions = (\n",
    "        vocab \n",
    "        | \"Parition items\" >> beam.Partition(_partition_fn, num_shards)\n",
    "    )\n",
    "    \n",
    "    return partitions\n",
    "\n",
    "    \n",
    "def create_top_pairs(index, item_frequency, top_count):\n",
    "    \n",
    "    def _compare(item_freq1, item_freq2):\n",
    "        _, freq1 = item_freq1\n",
    "        _, freq2 = item_freq2\n",
    "        return freq1 < freq2\n",
    "    \n",
    "    def _generate_pairs(items):\n",
    "        results = []\n",
    "        items = list(items)\n",
    "        count = len(items)\n",
    "        for i in range(count):\n",
    "            for j in range(i+1, count):\n",
    "                item1 = items[i][0]\n",
    "                item2 = items[j][0]\n",
    "                first, second = (min(item1, item2), max(item1, item2))\n",
    "                results.append((first, second, 0))\n",
    "        return results\n",
    "    \n",
    "    pairs = (\n",
    "        item_frequency\n",
    "        | \"Get top items - shard {}\".format(index) >> beam.CombineGlobally(\n",
    "            beam.combiners.TopCombineFn(top_count, _compare))\n",
    "        | \"Generate item pairs - shard {}\".format(index) >> beam.FlatMap(_generate_pairs)\n",
    "    )\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "def generate_and_union_pairs(partitions, top_count):\n",
    "    \n",
    "    pair_list = []\n",
    "    for i, partition in enumerate(partitions):\n",
    "        pairs = create_top_pairs(i+1, partition, top_count)\n",
    "        pair_list.append(pairs)\n",
    "            \n",
    "    pairs = (\n",
    "        pair_list\n",
    "        | \"Union item pairs\" >> beam.Flatten()\n",
    "    )\n",
    "    return pairs\n",
    "    \n",
    "def merge_pairs(pairs):\n",
    "    \n",
    "    def _pair_as_key(record):\n",
    "        item1, item2, cooc = record\n",
    "        return ((item1, item2), cooc)\n",
    "    \n",
    "    def _process_pair(record):\n",
    "        key, value = record\n",
    "        item1, item2 = key\n",
    "        cooc = max(value)\n",
    "        return (item1, item2, cooc)\n",
    "    \n",
    "    pairs = (\n",
    "        pairs\n",
    "        | \"Merge item pairs\" >> beam.Flatten()\n",
    "        | \"Use pair as key\" >> beam.Map(_pair_as_key)\n",
    "        | \"Group pairs by key\" >> beam.GroupByKey()\n",
    "        | \"Process pairs\" >> beam.Map(_process_pair)\n",
    "        \n",
    "    )\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def compute_score(data, total):\n",
    "    \n",
    "    def _compute_pmi(record, total):\n",
    "        import math\n",
    "        item1, item2, cooc, freq1, freq2 = record\n",
    "        \n",
    "        if cooc > 0:\n",
    "            pmi = math.log(cooc) - math.log(freq1) - math.log(freq2) + math.log(total)\n",
    "            weight = math.sqrt(cooc)\n",
    "            sample_type = 'P'\n",
    "        else:\n",
    "            pmi = math.log(1) - math.log(freq1) - math.log(freq2) + math.log(total)\n",
    "            weight = 1\n",
    "            sample_type = 'N'\n",
    "        return (item1, item2, round(pmi, 5), round(weight, 5), sample_type)\n",
    "    \n",
    "    \n",
    "    stats = (\n",
    "        data\n",
    "        | \"Compute pairewise mutual infromation\" >> beam.Map(_compute_pmi, beam.pvalue.AsSingleton(total))\n",
    "    )\n",
    "    return stats\n",
    "\n",
    "def get_info(stats):\n",
    "    \n",
    "    def _make_type_as_key(record):\n",
    "        _, _, _, _, record_type = record\n",
    "        return (record_type, 1)\n",
    "    \n",
    "    def _get_scores(record):\n",
    "        _, _, score, _, _ = record\n",
    "        return score\n",
    "    \n",
    "    counts = (\n",
    "        stats\n",
    "        | \"Group by record type\" >> beam.Map(_make_type_as_key)\n",
    "        | \"Count records\" >> beam.CombinePerKey(sum)\n",
    "        | \"Fromat counts\" >> beam.Map(lambda entry: '{}: {}'.format(entry[0], entry[1]))\n",
    "    )\n",
    "    \n",
    "    scores = (\n",
    "        stats\n",
    "        | \"Get scores\" >> beam.Map(_get_scores)\n",
    "    )\n",
    "    \n",
    "    mins = (\n",
    "        scores\n",
    "        | \"Get min score\" >> beam.CombineGlobally(min).without_defaults()\n",
    "        | \"Format min score\" >> beam.Map(lambda value: 'min: {}'.format(value))\n",
    "    )\n",
    "    \n",
    "    maxs = (\n",
    "        scores\n",
    "        | \"Get max score\" >> beam.CombineGlobally(max).without_defaults()\n",
    "        | \"Format max score\" >> beam.Map(lambda value: 'max: {}'.format(value))\n",
    "    )\n",
    "    \n",
    "    info = (\n",
    "        (counts, mins, maxs)\n",
    "        | \"Combine info\" >> beam.Flatten()\n",
    "    )\n",
    "    \n",
    "    return info\n",
    "    \n",
    "    \n",
    "def write_debug(data, sink_data_location):\n",
    "    \n",
    "    (\n",
    "        data\n",
    "        | 'Write debug' >> beam.io.WriteToText(\n",
    "            file_path_prefix = sink_data_location+\"/debug\")\n",
    "    )\n",
    "    \n",
    "\n",
    "def write_log(info, sink_data_location):\n",
    "    \n",
    "    (\n",
    "        info\n",
    "        | 'Write logs' >> beam.io.WriteToText(\n",
    "            file_path_prefix = sink_data_location+\"/info\",\n",
    "            file_name_suffix = \".log\",\n",
    "            shard_name_template ='',\n",
    "            num_shards = 1)\n",
    "    )\n",
    "\n",
    "def write_vocab(vocab, sink_data_location):\n",
    "    \n",
    "    (\n",
    "        vocab\n",
    "        | 'Write vocabulary file' >> beam.io.WriteToText(\n",
    "            file_path_prefix = sink_data_location+\"/vocab\", \n",
    "            file_name_suffix = \".txt\",\n",
    "            shard_name_template ='',\n",
    "            num_shards = 1)\n",
    "    )\n",
    "    \n",
    "\n",
    "def write_to_tfrecords(stats, sink_data_location):\n",
    "    \n",
    "    def _to_tf_example(record):\n",
    "        item1, item2, score, weight, record_type = record\n",
    "        feature = {\n",
    "            'item1': tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(value=[tf.compat.as_bytes(item1)])),\n",
    "            'item2': tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(value=[tf.compat.as_bytes(item2)])),\n",
    "            'score': tf.train.Feature(\n",
    "                float_list=tf.train.FloatList(value=[float(score)])),\n",
    "            'weight': tf.train.Feature(\n",
    "                float_list=tf.train.FloatList(value=[float(weight)])),\n",
    "            'type': tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(value=[tf.compat.as_bytes(record_type)])),\n",
    "        }\n",
    "        return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "        \n",
    "    (\n",
    "        stats\n",
    "        | 'Encode to tf.example' >> beam.Map(_to_tf_example)\n",
    "        | 'Serialize to string' >> beam.Map(lambda example: example.SerializeToString(deterministic=True))\n",
    "        | 'Write to TFRecords files' >> beam.io.WriteToTFRecord(\n",
    "                file_path_prefix = sink_data_location+\"/cooc\",\n",
    "                file_name_suffix = '.tfrecords')\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_text2cooc_pipeline(args):\n",
    "\n",
    "    source_data_location = args['source_data_location']\n",
    "    sink_data_location = args['sink_data_location']\n",
    "    runner = args['runner']\n",
    "    stop_words = args['stop_words']\n",
    "    min_freq = args['min_freq']\n",
    "    num_shards = args['num_shards']\n",
    "    top_count = args['top_count']\n",
    "    min_sentence_length = args['min_sentence_length']\n",
    "    window_size = args['window_size']\n",
    "    \n",
    "    pipeline_options = beam.options.pipeline_options.GoogleCloudOptions(**args)\n",
    "    \n",
    "    with beam.Pipeline(runner, options=pipeline_options) as pipeline:\n",
    "       \n",
    "        # Read text from source files as lines\n",
    "        lines = read_text(pipeline, source_data_location)\n",
    "        \n",
    "        # Tokenize lines into words\n",
    "        line_words = tokenize(lines, min_sentence_length, stop_words)\n",
    "\n",
    "        # Compute frequency of each word (word, frequency)\n",
    "        word_frequency = compute_word_frequency(line_words, min_freq)\n",
    "        #write_debug(word_frequency, sink_data_location)\n",
    "\n",
    "        # Extract distinct list of items (vocabulary)\n",
    "        vocab = vocabulary(word_frequency)\n",
    "        write_vocab(vocab, sink_data_location)\n",
    "        \n",
    "        # Generate pairs (item_1, item_2, cooc) for the top frequent items. cooc is set to 0.\n",
    "        top_pairs = create_top_pairs(0, word_frequency, top_count)\n",
    "        #write_debug(top_pairs, sink_data_location)\n",
    "        \n",
    "        # Split items to partitions\n",
    "        partitions = create_partitions(word_frequency, num_shards)\n",
    "        #write_debug(partitions[0], sink_data_location)\n",
    "        \n",
    "        # For each partition, generate  (item_1, item_2, score) for the top frequent items. Then union.\n",
    "        top_pairs_per_partition = generate_and_union_pairs(partitions, top_count)\n",
    "        #write_debug(pairs, sink_data_location)\n",
    "        \n",
    "        #For the co-occuring items, compute the cooccurrence (item_1, item_2, cooc)\n",
    "        cooccurrence = compute_cooccurrence(line_words, window_size)\n",
    "        #write_debug(cooccurrence, sink_data_location)\n",
    "        \n",
    "        # Merge all pairs: the co-occuring (positive) and not co-occuring (negative)\n",
    "        all_pairs = merge_pairs((cooccurrence, top_pairs, top_pairs_per_partition))\n",
    "        #write_debug(all_pairs, sink_data_location)\n",
    "        \n",
    "        # Compute |D| = \\sum_{ij} x_{ij} \n",
    "        total = compute_total_cooc(cooccurrence)\n",
    "        #write_debug(total, sink_data_location)\n",
    "        \n",
    "        # Join Cooc with item frequency => (item_1, item_2, cooc, freq_1, freq_2)\n",
    "        join = join_with_item_frequency(all_pairs, word_frequency)\n",
    "        #write_debug(join, sink_data_location)\n",
    "        \n",
    "        # Compute statistics (pmi), weights, and record type => (item_1, item_2, pmi, weight, type)\n",
    "        stats = compute_score(join, total)\n",
    "        #write_debug(stats, sink_data_location)\n",
    "        write_to_tfrecords(stats, sink_data_location)\n",
    "        \n",
    "        # Log information about the created dataset\n",
    "        info = get_info(stats)\n",
    "        write_log(info, sink_data_location)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = 'DirectRunner'\n",
    "job_name = 'text2cooc-{}'.format(datetime.utcnow().strftime('%y%m%d-%H%M%S'))\n",
    "\n",
    "args = {\n",
    "    'job_name': job_name,\n",
    "    'runner': runner,\n",
    "    'source_data_location': '{}/file.txt'.format(DATA_DIR),\n",
    "    'sink_data_location': COOC_DIR,\n",
    "    'min_freq': 5,\n",
    "    'top_count': 300,\n",
    "    'num_shards': 100,\n",
    "    'min_sentence_length': 3,\n",
    "    'window_size': 10,\n",
    "    'stop_words': stop_words,\n",
    "    'project': PROJECT_ID,\n",
    "}\n",
    "\n",
    "print(\"Pipeline args are set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = datetime.utcnow() \n",
    "print(\"Running pipeline...\")\n",
    "run_text2cooc_pipeline(args)\n",
    "print(\"Pipeline is done.\")\n",
    "time_end = datetime.utcnow() \n",
    "time_elapsed = time_end - time_start\n",
    "print(\"Execution elapsed time: {} seconds\".format(time_elapsed.total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {COOC_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l  {COOC_DIR}/vocab.txt\n",
    "!head {COOC_DIR}/vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head {COOC_DIR}/info.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Optional] Run on Cloud Dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'ksalama-cloudml'\n",
    "REGION = 'europe-west1'\n",
    "WORKSPACE = 'gs://ksalama-cloudml/text_workspace'\n",
    "DATA_DIR = '{}/corpus'.format(WORKSPACE)\n",
    "COOC_DIR = '{}/cooc'.format(WORKSPACE)\n",
    "STAGING_DIR ='{}/stg'.format(WORKSPACE)\n",
    "TEMP_DIR ='{}/tmp'.format(WORKSPACE)\n",
    "\n",
    "if tf.io.gfile.exists(COOC_DIR):\n",
    "    print(\"Removing {} contents...\".format(COOC_DIR))\n",
    "    tf.io.gfile.rmtree(COOC_DIR)\n",
    "\n",
    "print(\"Creating output: {}\".format(COOC_DIR))\n",
    "tf.io.gfile.makedirs(COOC_DIR)\n",
    "\n",
    "runner = 'DataflowRunner'\n",
    "job_name = 'text2cooc-{}'.format(datetime.utcnow().strftime('%y%m%d-%H%M%S'))\n",
    "\n",
    "args = {\n",
    "    'job_name': job_name,\n",
    "    'runner': runner,\n",
    "    'source_data_location': '{}/00*.txt'.format(DATA_DIR),\n",
    "    'sink_data_location': COOC_DIR,\n",
    "    'stop_words':  stop_words,\n",
    "    'min_freq': 100,\n",
    "    'top_count': 300,\n",
    "    'num_shards': 100,\n",
    "    'min_sentence_length': 3,\n",
    "    'window_size': 10,\n",
    "    'project': PROJECT_ID,\n",
    "    'region': REGION,\n",
    "    'staging_location': STAGING_DIR,\n",
    "    'temp_location': TEMP_DIR,\n",
    "    'save_main_session': True,\n",
    "}\n",
    "\n",
    "print(\"Pipeline args are set.\\n\")\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = datetime.utcnow() \n",
    "print(\"Running pipeline...\")\n",
    "run_text2cooc_pipeline(args)\n",
    "print(\"Pipeline is done.\")\n",
    "time_end = datetime.utcnow() \n",
    "time_elapsed = time_end - time_start\n",
    "print(\"Execution elapsed time: {} seconds\".format(time_elapsed.total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {COOC_DIR}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r ./workspace/cooc/\n",
    "!gsutil -m cp gs://ksalama-cloudml/text_workspace/cooc/info.log ./workspace/cooc/info.log\n",
    "!gsutil -m cp gs://ksalama-cloudml/text_workspace/cooc/vocab.txt ./workspace/cooc/vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head  ./workspace/cooc/info.log\n",
    "!wc -l   ./workspace/cooc/vocab.txt\n",
    "!head  ./workspace/cooc/vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Read TFRecords using tf.data APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil -m cp gs://ksalama-cloudml/text_workspace/cooc/cooc-00000-* ./workspace/cooc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_fn(file_pattern, batch_size):\n",
    "    \n",
    "    features = {\n",
    "        'item1': tf.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'item2': tf.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'score': tf.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "        'weight': tf.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "        'type': tf.FixedLenFeature(dtype=tf.string, shape=())\n",
    "    }\n",
    "\n",
    "    def _input_fn():\n",
    "        dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "            file_pattern,\n",
    "            batch_size,\n",
    "            features,\n",
    "            reader=tf.data.TFRecordDataset,\n",
    "            label_key=None,\n",
    "            num_epochs=1,\n",
    "            shuffle=True\n",
    "        )\n",
    "        return dataset\n",
    "    \n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()\n",
    "\n",
    "DATA_FILES = \"{}/cooc-*\".format(COOC_DIR)\n",
    "\n",
    "dataset = make_input_fn(DATA_FILES, batch_size=5)()\n",
    "for i, features in enumerate(dataset.take(5)):\n",
    "    print()\n",
    "    print(\"Record {}:\".format(i+1))\n",
    "    for key in features:\n",
    "        print(\"-{}:{}\".format(key, features[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
