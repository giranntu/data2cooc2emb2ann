{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Co-occurence Statistics from Text\n",
    "\n",
    "This tutorial shows how to compute **Co-occurrence statistics** for words in a given text corups.\n",
    "\n",
    "The statistics we compute for each co-occurring pair is the [Pointwise mutual information](https://en.wikipedia.org/wiki/Pointwise_mutual_information) (pmi), which is used by the [Swivel](https://arxiv.org/pdf/1602.02215.pdf) algorithm for learning embeddings.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/ksalama/data2cooc2emb2ann/blob/master/text2emb/01-Creating_Co-occurence_Statistics_from_Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import apache_beam as beam\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'ksalama-cloudml'\n",
    "REGION = 'europe-west1'\n",
    "WORKSPACE = './workspace'\n",
    "DATA_DIR = '{}/data'.format(WORKSPACE)\n",
    "COOC_DIR = '{}/cooc'.format(WORKSPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing ./workspace/cooc contents...\n",
      "Creating output: ./workspace/cooc\n"
     ]
    }
   ],
   "source": [
    "if tf.io.gfile.exists(COOC_DIR):\n",
    "    print(\"Removing {} contents...\".format(COOC_DIR))\n",
    "    tf.io.gfile.rmtree(COOC_DIR)\n",
    "\n",
    "print(\"Creating output: {}\".format(COOC_DIR))\n",
    "tf.io.gfile.makedirs(COOC_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \n",
    "             \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \n",
    "             \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \n",
    "             \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \n",
    "             \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \n",
    "             \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \n",
    "             \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \n",
    "             \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \n",
    "             \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \n",
    "             \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \n",
    "             \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \n",
    "             \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \n",
    "             \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \n",
    "             \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(pipeline, source_data_location):\n",
    "    lines = ( \n",
    "        pipeline\n",
    "        | 'Read from files'>> beam.io.ReadFromText(\n",
    "            file_pattern=source_data_location)\n",
    "        | 'Skip empty lines'>> beam.Filter(lambda line: len(line) > 2)\n",
    "    )\n",
    "    return lines\n",
    "\n",
    "\n",
    "def tokenize(lines, min_sentence_length, stop_words):\n",
    "\n",
    "    import string\n",
    "    \n",
    "    def _tokenize(line, stop_words):\n",
    "        line = ''.join(ch for ch in line if ch not in set(string.punctuation)).strip().lower()\n",
    "        tokens = [token for token in line.split() if len(token) > 2 and not token.isdigit()]\n",
    "        for word in list(stop_words):\n",
    "            if word in tokens:\n",
    "                tokens.remove(word)\n",
    "        return tokens\n",
    "    \n",
    "    def _valid(tokens, min_sentence_length):\n",
    "        return len(list(tokens)) >= min_sentence_length\n",
    "\n",
    "    \n",
    "    line_words = ( \n",
    "        lines\n",
    "        | 'Tokenize lines'>> beam.Map(_tokenize, stop_words)\n",
    "        | 'Filter invalid lines'>> beam.Filter(_valid, min_sentence_length)\n",
    "    )\n",
    "    return line_words\n",
    "\n",
    "\n",
    "def compute_word_frequency(line_words, min_freq):\n",
    "    \n",
    "    def _count_words(line_words):\n",
    "        from collections import Counter\n",
    "        word_counts = list(Counter(line_words).items())\n",
    "        return word_counts\n",
    "    \n",
    "    def _include(word_counts):\n",
    "        word, count = word_counts\n",
    "        return count >= min_freq\n",
    "    \n",
    "    word_counts = (\n",
    "        line_words\n",
    "        | 'Word count per line'>> beam.FlatMap(_count_words)\n",
    "        | 'Compute word frequency' >> beam.CombinePerKey(sum)\n",
    "        | 'Filter infrequent words' >> beam.Filter(_include)\n",
    "    )\n",
    "    return word_counts\n",
    "\n",
    "\n",
    "def compute_cooccurrence(line_words, window_size):\n",
    "    \n",
    "    from collections import defaultdict\n",
    "    \n",
    "    def _extract_coocs_from_line(line_words, window_size):\n",
    "        \n",
    "        coocs = defaultdict(float)\n",
    "        \n",
    "        for position, word1 in enumerate(line_words):\n",
    "            window_extent = min(window_size + 1, len(line_words) - position)\n",
    "            for offset in range(1, window_extent):\n",
    "                word2 = line_words[position + offset]\n",
    "                pair = (min(word1, word2), max(word1, word2))\n",
    "                count = 1.0 / offset\n",
    "                coocs[pair] += count\n",
    "\n",
    "            pair = (word1, word1)\n",
    "            coocs[pair] += 0.5  # Only add 1/2 since we output (a, b) and (b, a)\n",
    "        \n",
    "        return list(coocs.items())\n",
    "\n",
    "    def _format_output(record):\n",
    "        key, value = record\n",
    "        item_1, item_2 = key\n",
    "        return (item_1, item_2, value)\n",
    "    \n",
    "    cooccurrence = (\n",
    "        line_words\n",
    "        | 'Extract cooccurrence from line' >> beam.FlatMap(_extract_coocs_from_line, window_size)\n",
    "        | 'Aggregate cooccurrences' >> beam.CombinePerKey(sum)\n",
    "        | 'Format cooc' >> beam.Map(_format_output)\n",
    "        \n",
    "    )\n",
    "    return cooccurrence\n",
    "\n",
    "def compute_total_cooc(cooccurrence):\n",
    "    \n",
    "    def _get_cooc_values(record):\n",
    "        _, _, cooc = record\n",
    "        return cooc\n",
    "    \n",
    "    result = (\n",
    "        cooccurrence\n",
    "        | \"Get cooc values\" >> beam.Map(_get_cooc_values)\n",
    "        | \"Sum cooc values\" >> beam.CombineGlobally(sum)\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "def join_with_item_frequency(cooccurrence, item_frequency):\n",
    "    \n",
    "    def _make_item1_key(record):\n",
    "        item1, item2, cooc = record\n",
    "        return (item1, (item2, cooc))\n",
    "        \n",
    "    def _make_item2_key(record):\n",
    "        item1, item2, cooc, freq1 = record\n",
    "        return (item2, (item1, cooc, freq1))\n",
    "    \n",
    "    def _is_frequent_item(record):\n",
    "        _, entry = record\n",
    "        return len(list(entry['freq'])) == 1\n",
    "    \n",
    "    def _reformat1(record):\n",
    "        result = []\n",
    "        item1, entry = record\n",
    "        cooc_items = entry['cooc']\n",
    "        item1_freq = entry['freq']\n",
    "        \n",
    "        for item2, cooc in cooc_items:\n",
    "            result.append(\n",
    "                (item1, item2, cooc, item1_freq[0])\n",
    "            )\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def _reformat2(record):\n",
    "        result = []\n",
    "        item2, entry = record\n",
    "        cooc_items = entry['cooc']\n",
    "        item2_freq = entry['freq']\n",
    "        \n",
    "        for item1, cooc, freq1 in cooc_items:\n",
    "            result.append(\n",
    "                (item1, item2, cooc, freq1, item2_freq[0])\n",
    "            )\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    item1_cooc = (\n",
    "        cooccurrence\n",
    "        | \"Make item 1 as key\" >> beam.Map(_make_item1_key)\n",
    "    )\n",
    "    \n",
    "    cooc_and_freq1 = (\n",
    "        {'cooc': item1_cooc, 'freq': item_frequency}\n",
    "        | \"Join with item 1 frequency\" >> beam.CoGroupByKey()\n",
    "        | \"Filter frequent items 1\" >> beam.Filter(_is_frequent_item)\n",
    "        | \"Reformat\" >> beam.FlatMap(_reformat1)\n",
    "        \n",
    "    )\n",
    "    \n",
    "    item2_cooc = (\n",
    "        cooc_and_freq1\n",
    "        | \"Make item 2 as key\" >> beam.Map(_make_item2_key)\n",
    "    )\n",
    "    \n",
    "    stats = (\n",
    "        {'cooc': item2_cooc, 'freq': item_frequency}\n",
    "        | \"Join with item 2 frequency\" >> beam.CoGroupByKey()\n",
    "        | \"Filter frequent items 2\" >> beam.Filter(_is_frequent_item)\n",
    "        | \"Reformat again\" >> beam.FlatMap(_reformat2)\n",
    "    )\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "def vocabulary(item_frequency):\n",
    "    \n",
    "    def _get_vocab(record):\n",
    "        item, _ = record\n",
    "        return item\n",
    "    \n",
    "    vocab = (\n",
    "        item_frequency\n",
    "        | \"Extract item vocabulary\" >> beam.Map(_get_vocab)\n",
    "\n",
    "    )\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "def create_partitions(vocab, num_shards):\n",
    "    \n",
    "    def _partition_fn(item, num_shards):\n",
    "        return abs(hash(item)) % num_shards\n",
    "    \n",
    "    partitions = (\n",
    "        vocab \n",
    "        | \"Parition items\" >> beam.Partition(_partition_fn, num_shards)\n",
    "    )\n",
    "    \n",
    "    return partitions\n",
    "\n",
    "    \n",
    "def create_top_pairs(index, item_frequency, top_count):\n",
    "    \n",
    "    def _compare(item_freq1, item_freq2):\n",
    "        _, freq1 = item_freq1\n",
    "        _, freq2 = item_freq2\n",
    "        return freq1 < freq2\n",
    "    \n",
    "    def _generate_pairs(items):\n",
    "        results = []\n",
    "        items = list(items)\n",
    "        count = len(items)\n",
    "        for i in range(count):\n",
    "            for j in range(i+1, count):\n",
    "                item1 = items[i][0]\n",
    "                item2 = items[j][0]\n",
    "                first, second = (item1, item2) if  item1 < item2 else (item2, item1)\n",
    "                results.append((first, second, 0))\n",
    "        return results\n",
    "    \n",
    "    pairs = (\n",
    "        item_frequency\n",
    "        | \"Get top items - shard {}\".format(index) >> beam.CombineGlobally(\n",
    "            beam.combiners.TopCombineFn(top_count, _compare))\n",
    "        | \"Generate item pairs - shard {}\".format(index) >> beam.FlatMap(_generate_pairs)\n",
    "    )\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "def generate_and_union_pairs(partitions, top_count):\n",
    "    \n",
    "    pair_list = []\n",
    "    for i, partition in enumerate(partitions):\n",
    "        pairs = create_top_pairs(i+1, partition, top_count)\n",
    "        pair_list.append(pairs)\n",
    "            \n",
    "    pairs = (\n",
    "        pair_list\n",
    "        | \"Union item pairs\" >> beam.Flatten()\n",
    "    )\n",
    "    return pairs\n",
    "    \n",
    "def merge_pairs(pairs):\n",
    "    \n",
    "    def _pair_as_key(record):\n",
    "        item1, item2, cooc = record\n",
    "        return ((item1, item2), cooc)\n",
    "    \n",
    "    def _process_pair(record):\n",
    "        key, value = record\n",
    "        item1, item2 = key\n",
    "        cooc = max(value)\n",
    "        return (item1, item2, cooc)\n",
    "    \n",
    "    pairs = (\n",
    "        pairs\n",
    "        | \"Merge item pairs\" >> beam.Flatten()\n",
    "        | \"Use pair as key\" >> beam.Map(_pair_as_key)\n",
    "        | \"Group pairs by key\" >> beam.GroupByKey()\n",
    "        | \"Process pairs\" >> beam.Map(_process_pair)\n",
    "        \n",
    "    )\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def compute_score(data, total):\n",
    "    \n",
    "    def _compute_pmi(record, total):\n",
    "        import math\n",
    "        item1, item2, cooc, freq1, freq2 = record\n",
    "        \n",
    "        if cooc > 0:\n",
    "            pmi = math.log(cooc) - math.log(freq1) - math.log(freq2) + math.log(total)\n",
    "            weight = math.sqrt(cooc)\n",
    "            sample_type = 'P'\n",
    "        else:\n",
    "            pmi = math.log(1) - math.log(freq1) - math.log(freq2) + math.log(total)\n",
    "            weight = 1\n",
    "            sample_type = 'N'\n",
    "        return (item1, item2, round(pmi, 5), round(weight, 5), sample_type)\n",
    "    \n",
    "    \n",
    "    stats = (\n",
    "        data\n",
    "        | \"Compute pairewise mutual infromation\" >> beam.Map(_compute_pmi, beam.pvalue.AsSingleton(total))\n",
    "    )\n",
    "    return stats\n",
    "\n",
    "def get_info(stats):\n",
    "    \n",
    "    def _make_type_as_key(record):\n",
    "        _, _, _, _, record_type = record\n",
    "        return (record_type, 1)\n",
    "    \n",
    "    def _get_scores(record):\n",
    "        _, _, score, _, _ = record\n",
    "        return score\n",
    "    \n",
    "    counts = (\n",
    "        stats\n",
    "        | \"Group by record type\" >> beam.Map(_make_type_as_key)\n",
    "        | \"Count records\" >> beam.CombinePerKey(sum)\n",
    "        | \"Fromat counts\" >> beam.Map(lambda entry: '{}: {}'.format(entry[0], entry[1]))\n",
    "    )\n",
    "    \n",
    "    scores = (\n",
    "        stats\n",
    "        | \"Get scores\" >> beam.Map(_get_scores)\n",
    "    )\n",
    "    \n",
    "    mins = (\n",
    "        scores\n",
    "        | \"Get min score\" >> beam.CombineGlobally(min).without_defaults()\n",
    "        | \"Format min score\" >> beam.Map(lambda value: 'min: {}'.format(value))\n",
    "    )\n",
    "    \n",
    "    maxs = (\n",
    "        scores\n",
    "        | \"Get max score\" >> beam.CombineGlobally(max).without_defaults()\n",
    "        | \"Format max score\" >> beam.Map(lambda value: 'max: {}'.format(value))\n",
    "    )\n",
    "    \n",
    "    info = (\n",
    "        (counts, mins, maxs)\n",
    "        | \"Combine info\" >> beam.Flatten()\n",
    "    )\n",
    "    \n",
    "    return info\n",
    "    \n",
    "    \n",
    "def write_debug(data, sink_data_location):\n",
    "    \n",
    "    (\n",
    "        data\n",
    "        | 'Write debug' >> beam.io.WriteToText(\n",
    "            file_path_prefix = sink_data_location+\"/debug\")\n",
    "    )\n",
    "    \n",
    "\n",
    "def write_log(info, sink_data_location):\n",
    "    \n",
    "    (\n",
    "        info\n",
    "        | 'Write logs' >> beam.io.WriteToText(\n",
    "            file_path_prefix = sink_data_location+\"/info\",\n",
    "            file_name_suffix = \".log\",\n",
    "            shard_name_template ='',\n",
    "            num_shards = 1)\n",
    "    )\n",
    "\n",
    "def write_vocab(vocab, sink_data_location):\n",
    "    \n",
    "    (\n",
    "        vocab\n",
    "        | 'Write vocabulary file' >> beam.io.WriteToText(\n",
    "            file_path_prefix = sink_data_location+\"/vocab\", \n",
    "            file_name_suffix = \".txt\",\n",
    "            shard_name_template ='',\n",
    "            num_shards = 1)\n",
    "    )\n",
    "    \n",
    "\n",
    "def write_to_tfrecords(stats, sink_data_location):\n",
    "    \n",
    "    def _to_tf_example(record):\n",
    "        item1, item2, score, weight, record_type = record\n",
    "        feature = {\n",
    "            'item1': tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(value=[tf.compat.as_bytes(item1)])),\n",
    "            'item2': tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(value=[tf.compat.as_bytes(item2)])),\n",
    "            'score': tf.train.Feature(\n",
    "                float_list=tf.train.FloatList(value=[float(score)])),\n",
    "            'weight': tf.train.Feature(\n",
    "                float_list=tf.train.FloatList(value=[float(weight)])),\n",
    "            'type': tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(value=[tf.compat.as_bytes(record_type)])),\n",
    "        }\n",
    "        return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "        \n",
    "    (\n",
    "        stats\n",
    "        | 'Encode to tf.example' >> beam.Map(_to_tf_example)\n",
    "        | 'Serialize to string' >> beam.Map(lambda example: example.SerializeToString(deterministic=True))\n",
    "        | 'Write to TFRecords files' >> beam.io.WriteToTFRecord(\n",
    "                file_path_prefix = sink_data_location+\"/cooc\",\n",
    "                file_name_suffix = '.tfrecords')\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_text2cooc_pipeline(args):\n",
    "\n",
    "    source_data_location = args['source_data_location']\n",
    "    sink_data_location = args['sink_data_location']\n",
    "    runner = args['runner']\n",
    "    stop_words = args['stop_words']\n",
    "    min_freq = args['min_freq']\n",
    "    num_shards = args['num_shards']\n",
    "    top_count = args['top_count']\n",
    "    min_sentence_length = args['min_sentence_length']\n",
    "    window_size = args['window_size']\n",
    "    \n",
    "    pipeline_options = beam.options.pipeline_options.GoogleCloudOptions(**args)\n",
    "    \n",
    "    with beam.Pipeline(runner, options=pipeline_options) as pipeline:\n",
    "       \n",
    "        # Read text from source files as lines\n",
    "        lines = read_text(pipeline, source_data_location)\n",
    "        \n",
    "        # Tokenize lines into words\n",
    "        line_words = tokenize(lines, min_sentence_length, stop_words)\n",
    "\n",
    "        # Compute frequency of each word (word, frequency)\n",
    "        word_frequency = compute_word_frequency(line_words, min_freq)\n",
    "\n",
    "        # Extract distinct list of items (vocabulary)\n",
    "        vocab = vocabulary(word_frequency)\n",
    "        write_vocab(vocab, sink_data_location)\n",
    "        \n",
    "        # Generate pairs (item_1, item_2, cooc) for the top frequent items. cooc is set to 0.\n",
    "        top_pairs = create_top_pairs(0, word_frequency, top_count)\n",
    "        #write_debug(top_pairs, sink_data_location)\n",
    "        \n",
    "        # Split items to partitions\n",
    "        partitions = create_partitions(word_frequency, num_shards)\n",
    "        #write_debug(partitions[0], sink_data_location)\n",
    "        \n",
    "        # For each partition, generate  (item_1, item_2, score) for the top frequent items. Then union.\n",
    "        top_pairs_per_partition = generate_and_union_pairs(partitions, top_count)\n",
    "        #write_debug(pairs, sink_data_location)\n",
    "        \n",
    "        #For the co-occuring items, compute the cooccurrence (item_1, item_2, cooc)\n",
    "        cooccurrence = compute_cooccurrence(line_words, window_size)\n",
    "        #write_debug(cooccurrence, sink_data_location)\n",
    "        \n",
    "        # Merge all pairs: the co-occuring (positive) and not co-occuring (negative)\n",
    "        all_pairs = merge_pairs((cooccurrence, top_pairs, top_pairs_per_partition))\n",
    "        #write_debug(all_pairs, sink_data_location)\n",
    "        \n",
    "        # Compute |D| = \\sum_{ij} x_{ij} \n",
    "        total = compute_total_cooc(cooccurrence)\n",
    "        #write_debug(total, sink_data_location)\n",
    "        \n",
    "        # Join Cooc with item frequency => (item_1, item_2, cooc, freq_1, freq_2)\n",
    "        join = join_with_item_frequency(all_pairs, word_frequency)\n",
    "        #write_debug(join, sink_data_location)\n",
    "        \n",
    "        # Compute statistics (pmi), weights, and record type => (item_1, item_2, pmi, weight, type)\n",
    "        stats = compute_score(join, total)\n",
    "        #write_debug(stats, sink_data_location)\n",
    "        write_to_tfrecords(stats, sink_data_location)\n",
    "        \n",
    "        # Log information about the created dataset\n",
    "        info = get_info(stats)\n",
    "        write_log(info, sink_data_location)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline args are set.\n"
     ]
    }
   ],
   "source": [
    "runner = 'DirectRunner'\n",
    "job_name = 'text2cooc-{}'.format(datetime.utcnow().strftime('%y%m%d-%H%M%S'))\n",
    "\n",
    "args = {\n",
    "    'job_name': job_name,\n",
    "    'runner': runner,\n",
    "    'source_data_location': '{}/file.txt'.format(DATA_DIR),\n",
    "    'sink_data_location': COOC_DIR,\n",
    "    'min_freq': 5,\n",
    "    'top_count': 300,\n",
    "    'num_shards': 100,\n",
    "    'min_sentence_length': 3,\n",
    "    'window_size': 10,\n",
    "    'stop_words': stop_words,\n",
    "    'project': PROJECT_ID,\n",
    "}\n",
    "\n",
    "print(\"Pipeline args are set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pipeline...\n",
      "Pipeline is done.\n",
      "Execution elapsed time: 71.319038 seconds\n"
     ]
    }
   ],
   "source": [
    "time_start = datetime.utcnow() \n",
    "print(\"Running pipeline...\")\n",
    "run_text2cooc_pipeline(args)\n",
    "print(\"Pipeline is done.\")\n",
    "time_end = datetime.utcnow() \n",
    "time_elapsed = time_end - time_start\n",
    "print(\"Execution elapsed time: {} seconds\".format(time_elapsed.total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mbeam-temp-info-15578cd4e22311e982aa784f439392c6\u001b[m\u001b[m\r\n",
      "cooc-00000-of-00001.tfrecords\r\n",
      "info.log\r\n",
      "vocab.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls {COOC_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     651 ./workspace/cooc/vocab.txt\n",
      "free\n",
      "see\n",
      "brexit\n",
      "article\n",
      "may\n",
      "long\n",
      "september\n",
      "considered\n",
      "for\n",
      "discussion\n"
     ]
    }
   ],
   "source": [
    "!wc -l  {COOC_DIR}/vocab.txt\n",
    "!head {COOC_DIR}/vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max: 9.10769\r\n",
      "min: -1.43385\r\n",
      "P: 26511\r\n",
      "N: 33879\r\n"
     ]
    }
   ],
   "source": [
    "!head {COOC_DIR}/info.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Optional] Run on Cloud Dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing gs://ksalama-cloudml/text_workspace/cooc contents...\n",
      "Creating output: gs://ksalama-cloudml/text_workspace/cooc\n",
      "Pipeline args are set.\n",
      "\n",
      "{'job_name': 'text2cooc-190928-191029', 'runner': 'DataflowRunner', 'source_data_location': 'gs://ksalama-cloudml/text_workspace/corpus/00*.txt', 'sink_data_location': 'gs://ksalama-cloudml/text_workspace/cooc', 'stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now'], 'min_freq': 100, 'top_count': 300, 'num_shards': 100, 'min_sentence_length': 3, 'window_size': 10, 'project': 'ksalama-cloudml', 'region': 'europe-west1', 'staging_location': 'gs://ksalama-cloudml/text_workspace/stg', 'temp_location': 'gs://ksalama-cloudml/text_workspace/tmp', 'save_main_session': True}\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = 'ksalama-cloudml'\n",
    "REGION = 'europe-west1'\n",
    "WORKSPACE = 'gs://ksalama-cloudml/text_workspace'\n",
    "DATA_DIR = '{}/corpus'.format(WORKSPACE)\n",
    "COOC_DIR = '{}/cooc'.format(WORKSPACE)\n",
    "STAGING_DIR ='{}/stg'.format(WORKSPACE)\n",
    "TEMP_DIR ='{}/tmp'.format(WORKSPACE)\n",
    "\n",
    "if tf.io.gfile.exists(COOC_DIR):\n",
    "    print(\"Removing {} contents...\".format(COOC_DIR))\n",
    "    tf.io.gfile.rmtree(COOC_DIR)\n",
    "\n",
    "print(\"Creating output: {}\".format(COOC_DIR))\n",
    "tf.io.gfile.makedirs(COOC_DIR)\n",
    "\n",
    "runner = 'DataflowRunner'\n",
    "job_name = 'text2cooc-{}'.format(datetime.utcnow().strftime('%y%m%d-%H%M%S'))\n",
    "\n",
    "args = {\n",
    "    'job_name': job_name,\n",
    "    'runner': runner,\n",
    "    'source_data_location': '{}/00*.txt'.format(DATA_DIR),\n",
    "    'sink_data_location': COOC_DIR,\n",
    "    'stop_words':  stop_words,\n",
    "    'min_freq': 100,\n",
    "    'top_count': 300,\n",
    "    'num_shards': 100,\n",
    "    'min_sentence_length': 3,\n",
    "    'window_size': 10,\n",
    "    'project': PROJECT_ID,\n",
    "    'region': REGION,\n",
    "    'staging_location': STAGING_DIR,\n",
    "    'temp_location': TEMP_DIR,\n",
    "    'save_main_session': True,\n",
    "}\n",
    "\n",
    "print(\"Pipeline args are set.\\n\")\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Discarding unparseable args: ['-f', '/Users/khalidsalama/Library/Jupyter/runtime/kernel-3e678468-1224-4a2c-b342-07510af4335a.json']\n",
      "WARNING:root:Discarding unparseable args: ['-f', '/Users/khalidsalama/Library/Jupyter/runtime/kernel-3e678468-1224-4a2c-b342-07510af4335a.json']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline is done.\n",
      "Execution elapsed time: 6262.423778 seconds\n"
     ]
    }
   ],
   "source": [
    "time_start = datetime.utcnow() \n",
    "print(\"Running pipeline...\")\n",
    "run_text2cooc_pipeline(args)\n",
    "print(\"Pipeline is done.\")\n",
    "time_end = datetime.utcnow() \n",
    "time_elapsed = time_end - time_start\n",
    "print(\"Execution elapsed time: {} seconds\".format(time_elapsed.total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://ksalama-cloudml/text_workspace/cooc/\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00000-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00001-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00002-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00003-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00004-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00005-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00006-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00007-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00008-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00009-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00010-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00011-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00012-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00013-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00014-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00015-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00016-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00017-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00018-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00019-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00020-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00021-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00022-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00023-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00024-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00025-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00026-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00027-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00028-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00029-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00030-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00031-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00032-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00033-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00034-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00035-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00036-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00037-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00038-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00039-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00040-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00041-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00042-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00043-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00044-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00045-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00046-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00047-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00048-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/cooc-00049-of-00050.tfrecords\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/info.log\r\n",
      "gs://ksalama-cloudml/text_workspace/cooc/vocab.txt\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls {COOC_DIR}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://ksalama-cloudml/text_workspace/cooc/info.log...\n",
      "/ [1/1 files][   49.0 B/   49.0 B] 100% Done                                    \n",
      "Operation completed over 1 objects/49.0 B.                                       \n",
      "Copying gs://ksalama-cloudml/text_workspace/cooc/vocab.txt...\n",
      "/ [1/1 files][171.5 KiB/171.5 KiB] 100% Done                                    \n",
      "Operation completed over 1 objects/171.5 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!rm -r ./workspace/cooc/\n",
    "!gsutil -m cp gs://ksalama-cloudml/text_workspace/cooc/info.log ./workspace/cooc/info.log\n",
    "!gsutil -m cp gs://ksalama-cloudml/text_workspace/cooc/vocab.txt ./workspace/cooc/vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: 14925898\n",
      "max: 13.144\n",
      "min: -6.68408\n",
      "N: 2173299\n",
      "   21554 ./workspace/cooc/vocab.txt\n",
      "occupy\n",
      "dies\n",
      "chieftains\n",
      "longitude\n",
      "pouch\n",
      "qua\n",
      "detach\n",
      "eclac\n",
      "grants\n",
      "induce\n"
     ]
    }
   ],
   "source": [
    "!head  ./workspace/cooc/info.log\n",
    "!wc -l   ./workspace/cooc/vocab.txt\n",
    "!head  ./workspace/cooc/vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Read TFRecords using tf.data APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://ksalama-cloudml/text_workspace/cooc/cooc-00000-of-00050.tfrecords...\n",
      "| [1/1 files][ 66.3 MiB/ 66.3 MiB] 100% Done 869.7 KiB/s ETA 00:00:00           \n",
      "Operation completed over 1 objects/66.3 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil -m cp gs://ksalama-cloudml/text_workspace/cooc/cooc-00000-* ./workspace/cooc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_fn(file_pattern, batch_size):\n",
    "    \n",
    "    features = {\n",
    "        'item1': tf.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'item2': tf.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'score': tf.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "        'weight': tf.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "        'type': tf.FixedLenFeature(dtype=tf.string, shape=())\n",
    "    }\n",
    "\n",
    "    def _input_fn():\n",
    "        dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "            file_pattern,\n",
    "            batch_size,\n",
    "            features,\n",
    "            reader=tf.data.TFRecordDataset,\n",
    "            label_key=None,\n",
    "            num_epochs=1,\n",
    "            shuffle=True\n",
    "        )\n",
    "        return dataset\n",
    "    \n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Record 1:\n",
      "-item1:[b'pairs' b'weary' b'numa' b'eased' b'etta']\n",
      "-item2:[b'parlor' b'why' b'why' b'soul' b'hurrah']\n",
      "-score:[4.90263 4.36157 4.99825 3.01387 7.22294]\n",
      "-type:[b'P' b'P' b'P' b'P' b'N']\n",
      "-weight:[0.70711 1.      0.70711 0.57735 1.     ]\n",
      "\n",
      "Record 2:\n",
      "-item1:[b'caled' b'army' b'copying' b'following' b'blockquotewith']\n",
      "-item2:[b'soul' b'soul' b'hurrah' b'foothills' b'soul']\n",
      "-score:[4.86225 0.47357 4.79141 4.59135 3.00777]\n",
      "-type:[b'P' b'P' b'N' b'P' b'P']\n",
      "-weight:[1.41421 1.48645 1.      1.41421 1.     ]\n",
      "\n",
      "Record 3:\n",
      "-item1:[b'neat' b'weep' b'erect' b'rendezvous' b'reconcile']\n",
      "-item2:[b'parlor' b'why' b'hymn' b'temptation' b'soul']\n",
      "-score:[5.85778 5.57784 4.93118 5.27921 2.07281]\n",
      "-type:[b'P' b'P' b'N' b'P' b'P']\n",
      "-weight:[1.22474 1.22474 1.      0.57735 0.70711]\n",
      "\n",
      "Record 4:\n",
      "-item1:[b'itch' b'full' b'halo' b'force' b'fall']\n",
      "-item2:[b'soul' b'soul' b'soul' b'hymn' b'foothills']\n",
      "-score:[2.9012  2.15434 3.25087 0.99296 2.58481]\n",
      "-type:[b'P' b'P' b'P' b'P' b'P']\n",
      "-weight:[0.57735 5.16559 0.70711 0.5     0.5    ]\n",
      "\n",
      "Record 5:\n",
      "-item1:[b'hermitage' b'partner' b'ornaments' b'outstretched' b'provoke']\n",
      "-item2:[b'soul' b'soul' b'parlor' b'why' b'temptation']\n",
      "-score:[3.12708 2.75646 3.7119  6.27684 4.13469]\n",
      "-type:[b'P' b'P' b'P' b'N' b'P']\n",
      "-weight:[0.70711 1.29099 0.44721 1.      0.57735]\n"
     ]
    }
   ],
   "source": [
    "tf.enable_eager_execution()\n",
    "\n",
    "DATA_FILES = \"{}/cooc-*\".format(COOC_DIR)\n",
    "\n",
    "dataset = make_input_fn(DATA_FILES, batch_size=5)()\n",
    "for i, features in enumerate(dataset.take(5)):\n",
    "    print()\n",
    "    print(\"Record {}:\".format(i+1))\n",
    "    for key in features:\n",
    "        print(\"-{}:{}\".format(key, features[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
