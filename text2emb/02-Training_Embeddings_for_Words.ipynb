{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Embeddings for Words\n",
    "\n",
    "This tutorial shows how to learn **word embeddings** from co-occurrence statistics.\n",
    "\n",
    "We use a simple NN architecture, a long with the conditional cost function used by the [Swivel](https://arxiv.org/pdf/1602.02215.pdf) algorithm. \n",
    "\n",
    "The learnt embeddings are then extracted from the model and saved as TSV file.\n",
    "\n",
    "The following are the steps of this tutorial:\n",
    "\n",
    "\n",
    "1. Define input data metadata\n",
    "2. Implement data input function\n",
    "3. Create feature columns\n",
    "4. Create a custome estimator\n",
    "5. Define the train and evaluate experiment\n",
    "6. Set the experiment parameters\n",
    "7. Run the experiment\n",
    "8. Extract the learnt **word embeddings** from the model\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/ksalama/data2cooc2emb2ann/blob/master/text2emb/02-Training_Embeddings_for_Words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE = './workspace'\n",
    "COOC_DIR = '{}/cooc'.format(WORKSPACE)\n",
    "MODELS_DIR = '{}/models'.format(WORKSPACE)\n",
    "SEED = 19831060"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"Files:\"\n",
    "!ls {COOC_DIR}/\n",
    "!echo \"\"\n",
    "\n",
    "!echo \"info:\"\n",
    "!head {COOC_DIR}/info.log\n",
    "!echo \"\"\n",
    "\n",
    "!echo \"vocab file:\"\n",
    "!head {COOC_DIR}/vocab.txt\n",
    "!echo \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_SCHEMA = {\n",
    "    'item1': tf.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'item2': tf.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'score': tf.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'weight': tf.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'type': tf.FixedLenFeature(dtype=tf.string, shape=())\n",
    "}\n",
    "\n",
    "WEIGHT_FEATURE_NAME = 'weight'\n",
    "TARGET_FEATURE_NAME = 'score'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Data Input Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_fn(\n",
    "    file_pattern, batch_size=128, num_epochs=1, shuffle=False):\n",
    "\n",
    "    def _input_fn():\n",
    "        dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "            file_pattern,\n",
    "            batch_size,\n",
    "            features=FEATURES_SCHEMA,\n",
    "            label_key=TARGET_FEATURE_NAME,\n",
    "            reader=tf.data.TFRecordDataset,\n",
    "            shuffle_buffer_size=batch_size * 2,\n",
    "            num_epochs=num_epochs,\n",
    "            shuffle=shuffle,\n",
    "            sloppy_ordering=True,\n",
    "            drop_final_batch=False\n",
    "        )\n",
    "        return dataset\n",
    "    \n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_columns(embedding_size, vocab1_file, vocab2_file):\n",
    "    \n",
    "    feature_columns = []\n",
    "\n",
    "    feature_columns.append(\n",
    "        tf.feature_column.embedding_column(\n",
    "            tf.feature_column.categorical_column_with_vocabulary_file(\n",
    "                key='item1', \n",
    "                vocabulary_file=vocab1_file\n",
    "            ), \n",
    "            embedding_size\n",
    "        )\n",
    "    )\n",
    "\n",
    "    feature_columns.append(\n",
    "        tf.feature_column.embedding_column(\n",
    "            tf.feature_column.categorical_column_with_vocabulary_file(\n",
    "                key='item2', \n",
    "                vocabulary_file=vocab2_file\n",
    "            ), \n",
    "            embedding_size\n",
    "        )\n",
    "    )\n",
    "        \n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Custom Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(labels, predictions, weights, types):\n",
    "    \n",
    "    def _positive_sample_cost(errors, weights):\n",
    "        return 0.5 * weights * tf.math.square(errors)\n",
    "    \n",
    "    def _negative_sample_cost(errors, weights):\n",
    "        return weights * tf.math.log(1 + tf.exp(errors))\n",
    "    \n",
    "    errors = predictions - labels\n",
    "    \n",
    "    p_loss = _positive_sample_cost(errors, weights)\n",
    "    n_loss = _negative_sample_cost(errors, weights)\n",
    "    loss = tf.where(tf.equal(types, 'P'), p_loss, n_loss)\n",
    "    \n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "def model_fn(features, labels, mode, params):\n",
    "    \n",
    "    items1 = features['item1']\n",
    "    feature_columns = create_feature_columns(\n",
    "        params.embedding_size, params.vocab1_file, params.vocab2_file)\n",
    "    \n",
    "    item1_layer = tf.feature_column.input_layer(\n",
    "        features={'item1': items1}, feature_columns=[feature_columns[0]])\n",
    "    \n",
    "    if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "        items2 = features['item2']\n",
    "        item2_layer = tf.feature_column.input_layer(\n",
    "            features={'item2': items2}, feature_columns=[feature_columns[1]])\n",
    "        \n",
    "        dot_product = tf.keras.layers.Dot(axes=1)([item1_layer, item2_layer])\n",
    "        logits = (params.max_value - params.min_value) * tf.sigmoid(dot_product) + params.min_value \n",
    "\n",
    "    predictions = None\n",
    "    export_outputs = None\n",
    "    loss = None\n",
    "    train_op = None\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions =  item1_layer\n",
    "        export_outputs = {'predictions': tf.estimator.export.PredictOutput(predictions)}\n",
    "    else:\n",
    "        types = features['type']\n",
    "        weights = features[WEIGHT_FEATURE_NAME]\n",
    "        \n",
    "#         loss = tf.losses.mean_squared_error(\n",
    "#             labels=labels, \n",
    "#             predictions=tf.squeeze(logits), \n",
    "#             weights=weights, \n",
    "#         )\n",
    "\n",
    "        loss = compute_loss(\n",
    "            labels=labels, \n",
    "            predictions=tf.squeeze(logits), \n",
    "            weights=weights, \n",
    "            types=types\n",
    "        )\n",
    "        \n",
    "        train_op=tf.train.AdamOptimizer(params.learning_rate).minimize(\n",
    "            loss=loss, global_step=tf.train.get_global_step())\n",
    "        \n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=predictions,\n",
    "        export_outputs=export_outputs,\n",
    "        loss=loss,\n",
    "        train_op=train_op\n",
    "    )\n",
    "\n",
    "\n",
    "def create_estimator(params, run_config):\n",
    "    \n",
    "    estimator = tf.estimator.Estimator(\n",
    "        model_fn,\n",
    "        params=params,\n",
    "        config=run_config\n",
    "    )\n",
    "    \n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(params, run_config):\n",
    "    \n",
    "    train_data_files = params.train_data_files\n",
    "    eval_data_files = params.eval_data_files\n",
    "    \n",
    "    # TrainSpec ####################################\n",
    "    train_input_fn = make_input_fn(\n",
    "        train_data_files,\n",
    "        batch_size=params.batch_size,\n",
    "        num_epochs=None,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn = train_input_fn,\n",
    "        max_steps=params.traning_steps\n",
    "    )\n",
    "    ###############################################    \n",
    "    \n",
    "    # EvalSpec ####################################\n",
    "    eval_input_fn = make_input_fn(\n",
    "        eval_data_files,\n",
    "        batch_size=params.batch_size,\n",
    "    )\n",
    "\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        name=datetime.utcnow().strftime(\"%H%M%S\"),\n",
    "        input_fn = eval_input_fn,\n",
    "        steps=params.eval_steps,\n",
    "        start_delay_secs=0,\n",
    "        throttle_secs=params.eval_throttle_secs\n",
    "    )\n",
    "    ###############################################\n",
    "\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    \n",
    "    if tf.gfile.Exists(run_config.model_dir):\n",
    "        print(\"Removing previous artefacts...\")\n",
    "        tf.gfile.DeleteRecursively(run_config.model_dir)\n",
    "            \n",
    "    print(\"\")\n",
    "    estimator = create_estimator(params, run_config)\n",
    "    print(\"\")\n",
    "    \n",
    "    time_start = datetime.utcnow() \n",
    "    print(\"Experiment started at {}\".format(time_start.strftime(\"%H:%M:%S\")))\n",
    "    print(\".......................................\") \n",
    "\n",
    "    tf.estimator.train_and_evaluate(\n",
    "        estimator=estimator,\n",
    "        train_spec=train_spec, \n",
    "        eval_spec=eval_spec\n",
    "    )\n",
    "\n",
    "    time_end = datetime.utcnow() \n",
    "    print(\".......................................\")\n",
    "    print(\"Experiment finished at {}\".format(time_end.strftime(\"%H:%M:%S\")))\n",
    "    print(\"\")\n",
    "    time_elapsed = time_end - time_start\n",
    "    print(\"Experiment elapsed time: {} seconds\".format(time_elapsed.total_seconds()))\n",
    "    \n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'cooc2emb-01'\n",
    "model_dir = os.path.join(MODELS_DIR, MODEL_NAME)\n",
    "info_file = os.path.join(COOC_DIR, 'info.log')\n",
    "min_value = 15\n",
    "max_value = -5\n",
    "\n",
    "info_map = {}\n",
    "\n",
    "if os.path.exists(info_file):\n",
    "    try:\n",
    "        with open(info_file) as f:\n",
    "            for line in f.readlines():\n",
    "                key, value = line.split(\":\")\n",
    "                info_map[key] = float(value)\n",
    "        min_value = math.floor(info_map['min'])\n",
    "        max_value = math.ceil(info_map['max'])\n",
    "    except: pass\n",
    "    \n",
    "class HParams():\n",
    "    pass\n",
    "\n",
    "params  = HParams()\n",
    "params.train_data_files = \"{}/cooc-*.tfrecords\".format(COOC_DIR)\n",
    "params.eval_data_files = \"{}/cooc-*.tfrecords\".format(COOC_DIR)\n",
    "params.vocab1_file = os.path.join(COOC_DIR,'vocab.txt')\n",
    "params.vocab2_file = os.path.join(COOC_DIR,'vocab.txt')\n",
    "params.embedding_size = 64\n",
    "params.min_value = min_value\n",
    "params.max_value = max_value\n",
    "params.batch_size = 512\n",
    "params.traning_steps = 50000 \n",
    "params.learning_rate = 0.001\n",
    "params.eval_steps = 1\n",
    "params.eval_throttle_secs = 0\n",
    "\n",
    "print(vars(params))\n",
    "\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    tf_random_seed=SEED,\n",
    "    save_checkpoints_steps=1000,\n",
    "    keep_checkpoint_max=3,\n",
    "    model_dir=model_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e: print(e)\n",
    "else: print('No GPUs detected')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = run_experiment(params, run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on Cloud AI Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_NAME = 'text_cooc2emb_{}'.format(datetime.utcnow().strftime('%y%m%d_%H%M%S'))\n",
    "PACKAGE_PATH = 'cloud_trainer/trainer'\n",
    "MODULE_NAME = 'trainer.train'\n",
    "JOB_DIR = 'gs://ksalama-cloudml/text_workspace/jobs'\n",
    "REGION = 'europe-west1'\n",
    "\n",
    "!gcloud ai-platform jobs submit training {JOB_NAME} \\\n",
    "        --package-path={PACKAGE_PATH} \\\n",
    "        --module-name={MODULE_NAME} \\\n",
    "        --job-dir={JOB_DIR} \\\n",
    "        --region={REGION} \\\n",
    "        --config=cloud_trainer/config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Extract Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(model_dir, checkpoint):\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.import_meta_graph(os.path.join(model_dir, 'model.ckpt-{}.meta'.format(checkpoint)))\n",
    "        saver.restore(sess, os.path.join(model_dir, 'model.ckpt-{}'.format(checkpoint)))\n",
    "        graph = tf.get_default_graph()\n",
    "        weights_tensor = graph.get_tensor_by_name('input_layer_1/item2_embedding/embedding_weights:0')\n",
    "        weights = np.array(sess.run(weights_tensor))\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = params.traning_steps\n",
    "embeddings = extract_embeddings(model_dir, checkpoint)\n",
    "print(len(embeddings))\n",
    "print(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = os.path.join(COOC_DIR,'vocab.txt')\n",
    "output_path = os.path.join(WORKSPACE,'embeddings.tsv')\n",
    "\n",
    "def write_embeddings_to_tsv():\n",
    "    with open(output_path, 'w') as out_f:\n",
    "        with open(vocab_path) as vocab_f:\n",
    "            for index, item in enumerate(vocab_f):\n",
    "                embedding = embeddings[index]\n",
    "                print('\\t'.join([item.strip()] + [str(x) for x in embedding]), file=out_f)\n",
    "                \n",
    "write_embeddings_to_tsv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head {output_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
