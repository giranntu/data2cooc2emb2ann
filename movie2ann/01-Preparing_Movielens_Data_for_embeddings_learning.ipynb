{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Movielens Data for Embeddings Learning\n",
    "\n",
    "The following are the steps of this tutorial:\n",
    "\n",
    "\n",
    "1. Download Movielens data.\n",
    "2. Preprocess the data and store it as TFRecord files.\n",
    "3. Read the prepared data in the TFRecords using tf.data APIs\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/ksalama/data2cooc2emb2ann/blob/master/movie2ann/01-Preparing_Movielens_Data_for_embeddings_learning.ipynb\" target=\"_parent\">\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import apache_beam as beam\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE = './workspace'\n",
    "DATA_DIR = '{}/data'.format(WORKSPACE)\n",
    "COOC_DIR = '{}/cooc'.format(WORKSPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating workspace: ./workspace\n"
     ]
    }
   ],
   "source": [
    "if tf.io.gfile.exists(WORKSPACE):\n",
    "    print(\"Removing {} contents...\".format(WORKSPACE))\n",
    "    tf.io.gfile.rmtree(WORKSPACE)\n",
    "\n",
    "print(\"Creating workspace: {}\".format(WORKSPACE))\n",
    "tf.io.gfile.makedirs(WORKSPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ml-1m'\n",
    "! wget http://files.grouplens.org/datasets/movielens/{DATASET}.zip -P {DATA_DIR}/\n",
    "! unzip {DATA_DIR}/{DATASET}.zip -d {DATA_DIR}/\n",
    "data_file = os.path.join(DATA_DIR, '{}/ratings.dat'.format(DATASET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khalidsalama/Technology/GoogleCloud/GCP-Github/kfp-components/google/tf_hub/tabular2cooc/venv/lib/python3.6/site-packages/ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 1000209\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating  timestamp\n",
       "0        1      1193       5  978300760\n",
       "1        1       661       3  978302109\n",
       "2        1       914       3  978301968\n",
       "3        1      3408       4  978300275\n",
       "4        1      2355       5  978824291"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "ratings_data = pd.read_csv(data_file, sep=\"::\", names=header)\n",
    "print(\"Size: {}\".format(len(ratings_data)))\n",
    "ratings_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(pipeline, source_data_location):\n",
    "    raw_data = ( \n",
    "        pipeline\n",
    "        | 'Read from files'>> beam.io.ReadFromText(\n",
    "            file_pattern=source_data_location)\n",
    "    )\n",
    "    return raw_data\n",
    "    \n",
    "\n",
    "def parse_data(raw_data, delimiter):\n",
    "    \n",
    "    def _parse_csv(line, delimiter):\n",
    "        try:\n",
    "            item1, item2, score = line.split(delimiter)[:3]\n",
    "            return (item1, item2, score)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid file format. A delimited data with three values is expected.\")\n",
    "            \n",
    "    parsed_data = (\n",
    "        raw_data\n",
    "        | 'Parse to tuple' >> beam.Map(_parse_csv, delimiter)\n",
    "    \n",
    "    )\n",
    "    return parsed_data\n",
    "\n",
    "def vocabulary(parsed_data, item_index):\n",
    "    \n",
    "    def _extract_item(record, item_index):\n",
    "        return record[item_index]\n",
    "    \n",
    "    vocab = (\n",
    "        parsed_data\n",
    "        | 'Extract item {}'.format(item_index) >> beam.Map(_extract_item, item_index)\n",
    "        | 'Extract vocabulary of item {}'.format(item_index) >> beam.Distinct()\n",
    "    \n",
    "    )\n",
    "    return vocab \n",
    "\n",
    "\n",
    "def process_data(parsed_data):\n",
    "    \n",
    "    def _extend_record(record):\n",
    "        item1, item2, score = record\n",
    "        return (item1, item2, score, 1, 'P')\n",
    "       \n",
    "    processed_data = (\n",
    "        parsed_data\n",
    "        | 'Extend record' >> beam.Map(_extend_record)\n",
    "    \n",
    "    )\n",
    "    return processed_data\n",
    "\n",
    "def get_info(stats):\n",
    "    \n",
    "    def _make_type_as_key(record):\n",
    "        _, _, _, _, record_type = record\n",
    "        return (record_type, 1)\n",
    "    \n",
    "    def _get_scores(record):\n",
    "        _, _, score, _, _ = record\n",
    "        return score\n",
    "    \n",
    "    counts = (\n",
    "        stats\n",
    "        | \"Group by record type\" >> beam.Map(_make_type_as_key)\n",
    "        | \"Count records\" >> beam.CombinePerKey(sum)\n",
    "        | \"Fromat counts\" >> beam.Map(lambda entry: '{}: {}'.format(entry[0], entry[1]))\n",
    "    )\n",
    "    \n",
    "    scores = (\n",
    "        stats\n",
    "        | \"Get scores\" >> beam.Map(_get_scores)\n",
    "    )\n",
    "    \n",
    "    mins = (\n",
    "        scores\n",
    "        | \"Get min score\" >> beam.CombineGlobally(min).without_defaults()\n",
    "        | \"Format min score\" >> beam.Map(lambda value: 'min: {}'.format(value))\n",
    "    )\n",
    "    \n",
    "    maxs = (\n",
    "        scores\n",
    "        | \"Get max score\" >> beam.CombineGlobally(max).without_defaults()\n",
    "        | \"Format max score\" >> beam.Map(lambda value: 'max: {}'.format(value))\n",
    "    )\n",
    "    \n",
    "    info = (\n",
    "        (counts, mins, maxs)\n",
    "        | \"Combine info\" >> beam.Flatten()\n",
    "    )\n",
    "    \n",
    "    return info\n",
    "    \n",
    "\n",
    "def write_debug(data, sink_data_location):\n",
    "    \n",
    "    (\n",
    "        data\n",
    "        | 'Write debug' >> beam.io.WriteToText(\n",
    "            file_path_prefix = sink_data_location+\"/debug\")\n",
    "    )\n",
    "    \n",
    "\n",
    "def write_log(info, sink_data_location):\n",
    "    \n",
    "    (\n",
    "        info\n",
    "        | 'Write logs' >> beam.io.WriteToText(\n",
    "            file_path_prefix = sink_data_location+\"/info\",\n",
    "            file_name_suffix = \".log\",\n",
    "            shard_name_template ='',\n",
    "            num_shards = 1)\n",
    "    )\n",
    "\n",
    "def write_vocab(vocab, sink_data_location, item_index):\n",
    "    \n",
    "    (\n",
    "        vocab\n",
    "        | 'Write vocabulary file {}'.format(item_index) >> beam.io.WriteToText(\n",
    "            file_path_prefix = sink_data_location+\"/vocab\", \n",
    "            file_name_suffix = \"-{}.txt\".format(item_index),\n",
    "            shard_name_template ='',\n",
    "            num_shards = 1)\n",
    "    )\n",
    "    \n",
    "\n",
    "def write_to_tfrecords(stats, sink_data_location):\n",
    "    \n",
    "    def _to_tf_example(record):\n",
    "        item1, item2, score, weight, record_type = record\n",
    "        feature = {\n",
    "            'item1': tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(value=[tf.compat.as_bytes(item1)])),\n",
    "            'item2': tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(value=[tf.compat.as_bytes(item2)])),\n",
    "            'score': tf.train.Feature(\n",
    "                float_list=tf.train.FloatList(value=[float(score)])),\n",
    "            'weight': tf.train.Feature(\n",
    "                float_list=tf.train.FloatList(value=[float(weight)])),\n",
    "            'type': tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(value=[tf.compat.as_bytes(record_type)])),\n",
    "        }\n",
    "        return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "        \n",
    "    (\n",
    "        stats\n",
    "        | 'Encode to tf.example' >> beam.Map(_to_tf_example)\n",
    "        | 'Serialize to string' >> beam.Map(lambda example: example.SerializeToString(deterministic=True))\n",
    "        | 'Write to TFRecords files' >> beam.io.WriteToTFRecord(\n",
    "                file_path_prefix = sink_data_location+\"/cooc\",\n",
    "                file_name_suffix = '.tfrecords')\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_preproc_pipeline(args):\n",
    "\n",
    "    source_data_location = args['source_data_location']\n",
    "    sink_data_location = args['sink_data_location']\n",
    "    delimiter = args['delimiter']\n",
    "    \n",
    "    pipeline_options = beam.options.pipeline_options.GoogleCloudOptions(**args)\n",
    "    \n",
    "    with beam.Pipeline(runner, options=pipeline_options) as pipeline:\n",
    "        \n",
    "        # Read data from source files\n",
    "        raw_data = read_data(pipeline, source_data_location)\n",
    "        \n",
    "        # Parse data to (item_1, item_2, score)\n",
    "        parsed_data = parse_data(raw_data, delimiter)\n",
    "        \n",
    "        # Process data to (item_1, item_2, score, weight, type)\n",
    "        processed_data = process_data(parsed_data)\n",
    "        #write_debug(processed_data, sink_data_location)\n",
    "        \n",
    "        # Extract distinct list of items 1 (vocabulary)\n",
    "        vocab1 = vocabulary(parsed_data, 0)\n",
    "        write_vocab(vocab1, sink_data_location, 0)\n",
    "\n",
    "        # Extract distinct list of items 2 (vocabulary)\n",
    "        vocab2 = vocabulary(parsed_data, 1)\n",
    "        write_vocab(vocab2, sink_data_location, 1)\n",
    "        \n",
    "        # Write processed data to tfrecords\n",
    "        write_to_tfrecords(processed_data, sink_data_location)\n",
    "        \n",
    "        # Log information about the created dataset\n",
    "        info = get_info(processed_data)\n",
    "        write_log(info, sink_data_location)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline args are set.\n"
     ]
    }
   ],
   "source": [
    "runner = 'DirectRunner'\n",
    "job_name = 'test-cooc-{}'.format(datetime.utcnow().strftime('%y%m%d-%H%M%S'))\n",
    "\n",
    "args = {\n",
    "    'job_name': job_name,\n",
    "    'runner': runner,\n",
    "    'source_data_location': data_file,\n",
    "    'sink_data_location': COOC_DIR,\n",
    "    'delimiter': '::',\n",
    "    'num_shards': 100,\n",
    "}\n",
    "print(\"Pipeline args are set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running preproc pipeline...\n",
      "Pipeline is done.\n",
      "Execution elapsed time: 327.089793 seconds\n"
     ]
    }
   ],
   "source": [
    "time_start = datetime.utcnow() \n",
    "print(\"Running preproc pipeline...\")\n",
    "run_preproc_pipeline(args)\n",
    "print(\"Pipeline is done.\")\n",
    "time_end = datetime.utcnow() \n",
    "time_elapsed = time_end - time_start\n",
    "print(\"Execution elapsed time: {} seconds\".format(time_elapsed.total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mbeam-temp-cooc-0d852e3ae0b411e99210784f439392c6\u001b[m\u001b[m\r\n",
      "\u001b[34mbeam-temp-cooc-40a3b13ae0b411e9bdc1784f439392c6\u001b[m\u001b[m\r\n",
      "\u001b[34mbeam-temp-cooc-6f5f21bae0b411e9bcc5784f439392c6\u001b[m\u001b[m\r\n",
      "\u001b[34mbeam-temp-cooc-84e1c474e0b411e98006784f439392c6\u001b[m\u001b[m\r\n",
      "\u001b[34mbeam-temp-info-0d7daa9ae0b411e9b859784f439392c6\u001b[m\u001b[m\r\n",
      "\u001b[34mbeam-temp-info-40a11fe2e0b411e989b9784f439392c6\u001b[m\u001b[m\r\n",
      "cooc-00000-of-00001.tfrecords\r\n",
      "info.log\r\n",
      "vocab-0.txt\r\n",
      "vocab-1.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls {COOC_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max: 5\r\n",
      "P: 1000209\r\n",
      "min: 1\r\n"
     ]
    }
   ],
   "source": [
    "!head {COOC_DIR}/info.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Read TFRecords using tf.data APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_fn(file_pattern, batch_size):\n",
    "    \n",
    "    features = {\n",
    "        'item1': tf.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'item2': tf.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'score': tf.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "        'weight': tf.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "        'type': tf.FixedLenFeature(dtype=tf.string, shape=())\n",
    "    }\n",
    "\n",
    "    def _input_fn():\n",
    "        dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "            file_pattern,\n",
    "            batch_size,\n",
    "            features,\n",
    "            reader=tf.data.TFRecordDataset,\n",
    "            label_key=None,\n",
    "            num_epochs=1,\n",
    "            shuffle=True\n",
    "        )\n",
    "        return dataset\n",
    "    \n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/khalidsalama/Technology/GoogleCloud/GCP-Github/kfp-components/google/tf_hub/tabular2cooc/venv/lib/python3.6/site-packages/tensorflow/python/data/experimental/ops/readers.py:835: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/khalidsalama/Technology/GoogleCloud/GCP-Github/kfp-components/google/tf_hub/tabular2cooc/venv/lib/python3.6/site-packages/tensorflow/python/data/experimental/ops/readers.py:835: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Record 1:\n",
      "-item1:[b'62' b'59' b'36' b'62' b'62']\n",
      "-item2:[b'3481' b'1028' b'1376' b'2407' b'1267']\n",
      "-score:[4. 3. 3. 4. 4.]\n",
      "-type:[b'P' b'P' b'P' b'P' b'P']\n",
      "-weight:[1. 1. 1. 1. 1.]\n",
      "\n",
      "Record 2:\n",
      "-item1:[b'8' b'19' b'39' b'7' b'6']\n",
      "-item2:[b'1673' b'1265' b'2770' b'1573' b'1688']\n",
      "-score:[5. 4. 4. 4. 5.]\n",
      "-type:[b'P' b'P' b'P' b'P' b'P']\n",
      "-weight:[1. 1. 1. 1. 1.]\n",
      "\n",
      "Record 3:\n",
      "-item1:[b'68' b'18' b'22' b'66' b'53']\n",
      "-item2:[b'2908' b'1215' b'2302' b'661' b'1848']\n",
      "-score:[5. 5. 3. 2. 3.]\n",
      "-type:[b'P' b'P' b'P' b'P' b'P']\n",
      "-weight:[1. 1. 1. 1. 1.]\n",
      "\n",
      "Record 4:\n",
      "-item1:[b'48' b'10' b'46' b'26' b'53']\n",
      "-item2:[b'2396' b'1196' b'1717' b'315' b'764']\n",
      "-score:[4. 5. 5. 3. 5.]\n",
      "-type:[b'P' b'P' b'P' b'P' b'P']\n",
      "-weight:[1. 1. 1. 1. 1.]\n",
      "\n",
      "Record 5:\n",
      "-item1:[b'56' b'18' b'29' b'15' b'56']\n",
      "-item2:[b'2997' b'1013' b'3527' b'3773' b'2786']\n",
      "-score:[4. 3. 5. 2. 1.]\n",
      "-type:[b'P' b'P' b'P' b'P' b'P']\n",
      "-weight:[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "tf.enable_eager_execution()\n",
    "\n",
    "DATA_FILES = \"{}/cooc-*\".format(COOC_DIR)\n",
    "\n",
    "dataset = make_input_fn(DATA_FILES, batch_size=5)()\n",
    "for i, features in enumerate(dataset.take(5)):\n",
    "    print()\n",
    "    print(\"Record {}:\".format(i+1))\n",
    "    for key in features:\n",
    "        print(\"-{}:{}\".format(key, features[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
