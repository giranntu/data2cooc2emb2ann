{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Movielens Data for Embeddings Learning\n",
    "\n",
    "The following are the steps of this tutorial:\n",
    "\n",
    "\n",
    "1. Download Movielens data.\n",
    "2. Preprocess the data and store it as TFRecord files.\n",
    "3. Read the prepared data in the TFRecords using tf.data APIs\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/ksalama/data2cooc2emb2ann/blob/master/movie2ann/01-Preparing_Movielens_Data_for_embeddings_learning.ipynb\" target=\"_parent\">\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: apache_beam[gcp]==2.14.0 in /opt/conda/lib/python3.7/site-packages (2.14.0)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /opt/conda/lib/python3.7/site-packages (from apache_beam[gcp]==2.14.0) (1.7)\n",
      "Requirement already satisfied: oauth2client<4,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from apache_beam[gcp]==2.14.0) (3.0.0)\n",
      "Requirement already satisfied: protobuf<4,>=3.5.0.post1 in /opt/conda/lib/python3.7/site-packages (from apache_beam[gcp]==2.14.0) (3.16.0)\n",
      "Requirement already satisfied: httplib2<=0.12.0,>=0.8 in /opt/conda/lib/python3.7/site-packages (from apache_beam[gcp]==2.14.0) (0.12.0)\n",
      "Requirement already satisfied: fastavro<0.22,>=0.21.4 in /opt/conda/lib/python3.7/site-packages (from apache_beam[gcp]==2.14.0) (0.21.24)\n",
      "Collecting pyarrow<0.15.0,>=0.11.1\n",
      "  Using cached pyarrow-0.14.1-cp37-cp37m-manylinux2010_x86_64.whl (58.1 MB)\n",
      "Requirement already satisfied: future<1.0.0,>=0.16.0 in /opt/conda/lib/python3.7/site-packages (from apache_beam[gcp]==2.14.0) (0.18.2)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from apache_beam[gcp]==2.14.0) (2.6.0)\n",
      "Requirement already satisfied: pydot<2,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from apache_beam[gcp]==2.14.0) (1.4.2)\n",
      "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from apache_beam[gcp]==2.14.0) (3.12.0)\n",
      "Requirement already satisfied: avro-python3<2.0.0,>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from apache_beam[gcp]==2.14.0) (1.9.2.1)\n",
      "Requirement already satisfied: dill<0.2.10,>=0.2.9 in /opt/conda/lib/python3.7/site-packages (from apache_beam[gcp]==2.14.0) (0.2.9)\n",
      "Requirement already satisfied: grpcio<2,>=1.8 in /home/jupyter/.local/lib/python3.7/site-packages (from apache_beam[gcp]==2.14.0) (1.40.0)\n",
      "Collecting pyyaml<4.0.0,>=3.12\n",
      "  Using cached PyYAML-3.13-cp37-cp37m-linux_x86_64.whl\n",
      "Requirement already satisfied: pytz>=2018.3 in /opt/conda/lib/python3.7/site-packages (from apache_beam[gcp]==2.14.0) (2021.1)\n",
      "Requirement already satisfied: mock<3.0.0,>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from apache_beam[gcp]==2.14.0) (2.0.0)\n",
      "Requirement already satisfied: google-cloud-core<0.30.0,>=0.28.1 in /opt/conda/lib/python3.7/site-packages (from apache_beam[gcp]==2.14.0) (0.29.1)\n",
      "Collecting google-cloud-bigtable<0.33.0,>=0.31.1\n",
      "  Using cached google_cloud_bigtable-0.32.2-py2.py3-none-any.whl (156 kB)\n",
      "Requirement already satisfied: google-cloud-bigquery<1.7.0,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from apache_beam[gcp]==2.14.0) (1.6.2)\n",
      "Requirement already satisfied: google-cloud-pubsub<0.40.0,>=0.39.0 in /opt/conda/lib/python3.7/site-packages (from apache_beam[gcp]==2.14.0) (0.39.1)\n",
      "Requirement already satisfied: google-apitools<0.5.29,>=0.5.28 in /opt/conda/lib/python3.7/site-packages (from apache_beam[gcp]==2.14.0) (0.5.28)\n",
      "Requirement already satisfied: cachetools<4,>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from apache_beam[gcp]==2.14.0) (3.1.1)\n",
      "Collecting google-cloud-datastore<1.8.0,>=1.7.1\n",
      "  Using cached google_cloud_datastore-1.7.4-py2.py3-none-any.whl (82 kB)\n",
      "Requirement already satisfied: fasteners>=0.14 in /opt/conda/lib/python3.7/site-packages (from google-apitools<0.5.29,>=0.5.28->apache_beam[gcp]==2.14.0) (0.16.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from google-apitools<0.5.29,>=0.5.28->apache_beam[gcp]==2.14.0) (1.16.0)\n",
      "Requirement already satisfied: google-resumable-media<0.5.0dev,>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<1.7.0,>=1.6.0->apache_beam[gcp]==2.14.0) (0.4.1)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<1.7.0,>=1.6.0->apache_beam[gcp]==2.14.0) (1.31.2)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.0.0->google-cloud-bigquery<1.7.0,>=1.6.0->apache_beam[gcp]==2.14.0) (1.35.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.0.0->google-cloud-bigquery<1.7.0,>=1.6.0->apache_beam[gcp]==2.14.0) (1.53.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.0.0->google-cloud-bigquery<1.7.0,>=1.6.0->apache_beam[gcp]==2.14.0) (2.25.1)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.0.0->google-cloud-bigquery<1.7.0,>=1.6.0->apache_beam[gcp]==2.14.0) (21.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.0.0->google-cloud-bigquery<1.7.0,>=1.6.0->apache_beam[gcp]==2.14.0) (57.4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core<2.0.0dev,>=1.0.0->google-cloud-bigquery<1.7.0,>=1.6.0->apache_beam[gcp]==2.14.0) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core<2.0.0dev,>=1.0.0->google-cloud-bigquery<1.7.0,>=1.6.0->apache_beam[gcp]==2.14.0) (4.7.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<0.12dev,>=0.11.4 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigtable<0.33.0,>=0.31.1->apache_beam[gcp]==2.14.0) (0.11.4)\n",
      "Requirement already satisfied: docopt in /opt/conda/lib/python3.7/site-packages (from hdfs<3.0.0,>=2.1.0->apache_beam[gcp]==2.14.0) (0.6.2)\n",
      "Requirement already satisfied: pbr>=0.11 in /opt/conda/lib/python3.7/site-packages (from mock<3.0.0,>=1.0.1->apache_beam[gcp]==2.14.0) (5.6.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from oauth2client<4,>=2.0.1->apache_beam[gcp]==2.14.0) (0.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-api-core<2.0.0dev,>=1.0.0->google-cloud-bigquery<1.7.0,>=1.6.0->apache_beam[gcp]==2.14.0) (2.4.7)\n",
      "Requirement already satisfied: numpy>=1.14 in /opt/conda/lib/python3.7/site-packages (from pyarrow<0.15.0,>=0.11.1->apache_beam[gcp]==2.14.0) (1.19.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.0.0->google-cloud-bigquery<1.7.0,>=1.6.0->apache_beam[gcp]==2.14.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.0.0->google-cloud-bigquery<1.7.0,>=1.6.0->apache_beam[gcp]==2.14.0) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.0.0->google-cloud-bigquery<1.7.0,>=1.6.0->apache_beam[gcp]==2.14.0) (1.26.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.0.0->google-cloud-bigquery<1.7.0,>=1.6.0->apache_beam[gcp]==2.14.0) (4.0.0)\n",
      "Installing collected packages: pyyaml, pyarrow, google-cloud-datastore, google-cloud-bigtable\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 5.4.1\n",
      "    Uninstalling PyYAML-5.4.1:\n",
      "      Successfully uninstalled PyYAML-5.4.1\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 2.0.0\n",
      "    Uninstalling pyarrow-2.0.0:\n",
      "      Successfully uninstalled pyarrow-2.0.0\n",
      "  Attempting uninstall: google-cloud-datastore\n",
      "    Found existing installation: google-cloud-datastore 1.15.3\n",
      "    Uninstalling google-cloud-datastore-1.15.3:\n",
      "      Successfully uninstalled google-cloud-datastore-1.15.3\n",
      "  Attempting uninstall: google-cloud-bigtable\n",
      "    Found existing installation: google-cloud-bigtable 1.7.0\n",
      "    Uninstalling google-cloud-bigtable-1.7.0:\n",
      "      Successfully uninstalled google-cloud-bigtable-1.7.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tfx-bsl 1.3.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.13.0 which is incompatible.\n",
      "tfx-bsl 1.3.0 requires apache-beam[gcp]<3,>=2.31, but you have apache-beam 2.14.0 which is incompatible.\n",
      "tfx-bsl 1.3.0 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.19.1 which is incompatible.\n",
      "tfx-bsl 1.3.0 requires pyarrow<3,>=1, but you have pyarrow 0.14.1 which is incompatible.\n",
      "tensorflow-transform 1.3.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.13.0 which is incompatible.\n",
      "tensorflow-transform 1.3.0 requires apache-beam[gcp]<3,>=2.31, but you have apache-beam 2.14.0 which is incompatible.\n",
      "tensorflow-transform 1.3.0 requires pyarrow<3,>=1, but you have pyarrow 0.14.1 which is incompatible.\n",
      "pandas-profiling 3.0.0 requires PyYAML>=5.0.0, but you have pyyaml 3.13 which is incompatible.\n",
      "libcst 0.3.20 requires pyyaml>=5.2, but you have pyyaml 3.13 which is incompatible.\n",
      "kubernetes 18.20.0 requires pyyaml>=5.4.1, but you have pyyaml 3.13 which is incompatible.\u001b[0m\n",
      "Successfully installed google-cloud-bigtable-0.32.2 google-cloud-datastore-1.7.4 pyarrow-0.14.1 pyyaml-3.13\n"
     ]
    }
   ],
   "source": [
    "!pip3 install apache_beam[gcp]==2.14.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/apache_beam/__init__.py:84: UserWarning: Some syntactic constructs of Python 3 are not yet fully supported by Apache Beam.\n",
      "  'Some syntactic constructs of Python 3 are not yet fully supported by '\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import apache_beam as beam\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE = './workspace'\n",
    "DATA_DIR = '{}/data'.format(WORKSPACE)\n",
    "COOC_DIR = '{}/cooc'.format(WORKSPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-09-23 15:24:07--  http://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
      "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
      "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5917549 (5.6M) [application/zip]\n",
      "Saving to: ‘./workspace/data/ml-1m.zip’\n",
      "\n",
      "ml-1m.zip           100%[===================>]   5.64M  32.3MB/s    in 0.2s    \n",
      "\n",
      "2021-09-23 15:24:08 (32.3 MB/s) - ‘./workspace/data/ml-1m.zip’ saved [5917549/5917549]\n",
      "\n",
      "Archive:  ./workspace/data/ml-1m.zip\n",
      "   creating: ./workspace/data/ml-1m/\n",
      "  inflating: ./workspace/data/ml-1m/movies.dat  \n",
      "  inflating: ./workspace/data/ml-1m/ratings.dat  \n",
      "  inflating: ./workspace/data/ml-1m/README  \n",
      "  inflating: ./workspace/data/ml-1m/users.dat  \n"
     ]
    }
   ],
   "source": [
    "DATASET = 'ml-1m'\n",
    "! wget http://files.grouplens.org/datasets/movielens/{DATASET}.zip -P {DATA_DIR}/\n",
    "! unzip {DATA_DIR}/{DATASET}.zip -d {DATA_DIR}/\n",
    "data_file = os.path.join(DATA_DIR, '{}/ratings.dat'.format(DATASET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 1000209\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating  timestamp\n",
       "0        1      1193       5  978300760\n",
       "1        1       661       3  978302109\n",
       "2        1       914       3  978301968\n",
       "3        1      3408       4  978300275\n",
       "4        1      2355       5  978824291"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "ratings_data = pd.read_csv(data_file, sep=\"::\", names=header)\n",
    "print(\"Size: {}\".format(len(ratings_data)))\n",
    "ratings_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(pipeline, source_data_location):\n",
    "    raw_data = ( \n",
    "        pipeline\n",
    "        | 'Read from files'>> beam.io.ReadFromText(\n",
    "            file_pattern=source_data_location)\n",
    "    )\n",
    "    return raw_data\n",
    "    \n",
    "\n",
    "def parse_data(raw_data, delimiter):\n",
    "    \n",
    "    def _parse_csv(line, delimiter):\n",
    "        try:\n",
    "            item1, item2, score = line.split(delimiter)[:3]\n",
    "            return (item1, item2, score)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid file format. A delimited data with three values is expected.\")\n",
    "            \n",
    "    parsed_data = (\n",
    "        raw_data\n",
    "        | 'Parse to tuple' >> beam.Map(_parse_csv, delimiter)\n",
    "    \n",
    "    )\n",
    "    return parsed_data\n",
    "\n",
    "def vocabulary(parsed_data, item_index):\n",
    "    \n",
    "    def _extract_item(record, item_index):\n",
    "        return record[item_index]\n",
    "    \n",
    "    vocab = (\n",
    "        parsed_data\n",
    "        | 'Extract item {}'.format(item_index) >> beam.Map(_extract_item, item_index)\n",
    "        | 'Extract vocabulary of item {}'.format(item_index) >> beam.Distinct()\n",
    "    \n",
    "    )\n",
    "    return vocab \n",
    "\n",
    "\n",
    "def process_data(parsed_data):\n",
    "    \n",
    "    def _extend_record(record):\n",
    "        item1, item2, score = record\n",
    "        return (item1, item2, score, 1, 'P')\n",
    "       \n",
    "    processed_data = (\n",
    "        parsed_data\n",
    "        | 'Extend record' >> beam.Map(_extend_record)\n",
    "    \n",
    "    )\n",
    "    return processed_data\n",
    "\n",
    "def get_info(stats):\n",
    "    \n",
    "    def _make_type_as_key(record):\n",
    "        _, _, _, _, record_type = record\n",
    "        return (record_type, 1)\n",
    "    \n",
    "    def _get_scores(record):\n",
    "        _, _, score, _, _ = record\n",
    "        return score\n",
    "    \n",
    "    counts = (\n",
    "        stats\n",
    "        | \"Group by record type\" >> beam.Map(_make_type_as_key)\n",
    "        | \"Count records\" >> beam.CombinePerKey(sum)\n",
    "        | \"Fromat counts\" >> beam.Map(lambda entry: '{}: {}'.format(entry[0], entry[1]))\n",
    "    )\n",
    "    \n",
    "    scores = (\n",
    "        stats\n",
    "        | \"Get scores\" >> beam.Map(_get_scores)\n",
    "    )\n",
    "    \n",
    "    mins = (\n",
    "        scores\n",
    "        | \"Get min score\" >> beam.CombineGlobally(min).without_defaults()\n",
    "        | \"Format min score\" >> beam.Map(lambda value: 'min: {}'.format(value))\n",
    "    )\n",
    "    \n",
    "    maxs = (\n",
    "        scores\n",
    "        | \"Get max score\" >> beam.CombineGlobally(max).without_defaults()\n",
    "        | \"Format max score\" >> beam.Map(lambda value: 'max: {}'.format(value))\n",
    "    )\n",
    "    \n",
    "    info = (\n",
    "        (counts, mins, maxs)\n",
    "        | \"Combine info\" >> beam.Flatten()\n",
    "    )\n",
    "    \n",
    "    return info\n",
    "    \n",
    "\n",
    "def write_debug(data, sink_data_location):\n",
    "    \n",
    "    (\n",
    "        data\n",
    "        | 'Write debug' >> beam.io.WriteToText(\n",
    "            file_path_prefix = sink_data_location+\"/debug\")\n",
    "    )\n",
    "    \n",
    "\n",
    "def write_log(info, sink_data_location):\n",
    "    \n",
    "    (\n",
    "        info\n",
    "        | 'Write logs' >> beam.io.WriteToText(\n",
    "            file_path_prefix = sink_data_location+\"/info\",\n",
    "            file_name_suffix = \".log\",\n",
    "            shard_name_template ='',\n",
    "            num_shards = 1)\n",
    "    )\n",
    "\n",
    "def write_vocab(vocab, sink_data_location, item_index):\n",
    "    \n",
    "    (\n",
    "        vocab\n",
    "        | 'Write vocabulary file {}'.format(item_index) >> beam.io.WriteToText(\n",
    "            file_path_prefix = sink_data_location+\"/vocab\", \n",
    "            file_name_suffix = \"-{}.txt\".format(item_index),\n",
    "            shard_name_template ='',\n",
    "            num_shards = 1)\n",
    "    )\n",
    "    \n",
    "\n",
    "def write_to_tfrecords(stats, sink_data_location):\n",
    "    \n",
    "    def _to_tf_example(record):\n",
    "        item1, item2, score, weight, record_type = record\n",
    "        feature = {\n",
    "            'item1': tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(value=[tf.compat.as_bytes(item1)])),\n",
    "            'item2': tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(value=[tf.compat.as_bytes(item2)])),\n",
    "            'score': tf.train.Feature(\n",
    "                float_list=tf.train.FloatList(value=[float(score)])),\n",
    "            'weight': tf.train.Feature(\n",
    "                float_list=tf.train.FloatList(value=[float(weight)])),\n",
    "            'type': tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(value=[tf.compat.as_bytes(record_type)])),\n",
    "        }\n",
    "        return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "        \n",
    "    (\n",
    "        stats\n",
    "        | 'Encode to tf.example' >> beam.Map(_to_tf_example)\n",
    "        | 'Serialize to string' >> beam.Map(lambda example: example.SerializeToString(deterministic=True))\n",
    "        | 'Write to TFRecords files' >> beam.io.WriteToTFRecord(\n",
    "                file_path_prefix = sink_data_location+\"/cooc\",\n",
    "                file_name_suffix = '.tfrecords')\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_preproc_pipeline(args):\n",
    "\n",
    "    source_data_location = args['source_data_location']\n",
    "    sink_data_location = args['sink_data_location']\n",
    "    delimiter = args['delimiter']\n",
    "    \n",
    "    pipeline_options = beam.options.pipeline_options.GoogleCloudOptions(**args)\n",
    "    \n",
    "    with beam.Pipeline(runner, options=pipeline_options) as pipeline:\n",
    "        \n",
    "        # Read data from source files\n",
    "        raw_data = read_data(pipeline, source_data_location)\n",
    "        \n",
    "        # Parse data to (item_1, item_2, score)\n",
    "        parsed_data = parse_data(raw_data, delimiter)\n",
    "        \n",
    "        # Process data to (item_1, item_2, score, weight, type)\n",
    "        processed_data = process_data(parsed_data)\n",
    "        #write_debug(processed_data, sink_data_location)\n",
    "        \n",
    "        # Extract distinct list of items 1 (vocabulary)\n",
    "        vocab1 = vocabulary(parsed_data, 0)\n",
    "        write_vocab(vocab1, sink_data_location, 0)\n",
    "\n",
    "        # Extract distinct list of items 2 (vocabulary)\n",
    "        vocab2 = vocabulary(parsed_data, 1)\n",
    "        write_vocab(vocab2, sink_data_location, 1)\n",
    "        \n",
    "        # Write processed data to tfrecords\n",
    "        write_to_tfrecords(processed_data, sink_data_location)\n",
    "        \n",
    "        # Log information about the created dataset\n",
    "        info = get_info(processed_data)\n",
    "        write_log(info, sink_data_location)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline args are set.\n"
     ]
    }
   ],
   "source": [
    "runner = 'DirectRunner'\n",
    "job_name = 'test-cooc-{}'.format(datetime.utcnow().strftime('%y%m%d-%H%M%S'))\n",
    "\n",
    "args = {\n",
    "    'job_name': job_name,\n",
    "    'runner': runner,\n",
    "    'source_data_location': data_file,\n",
    "    'sink_data_location': COOC_DIR,\n",
    "    'delimiter': '::',\n",
    "    'num_shards': 100,\n",
    "}\n",
    "print(\"Pipeline args are set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating workspace: ./workspace\n",
      "Running preproc pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline is done.\n",
      "Execution elapsed time: 99.042967 seconds\n"
     ]
    }
   ],
   "source": [
    "# if tf.io.gfile.exists(WORKSPACE):\n",
    "#     print(\"Removing {} contents...\".format(WORKSPACE))\n",
    "#     tf.io.gfile.rmtree(WORKSPACE)\n",
    "\n",
    "print(\"Creating workspace: {}\".format(WORKSPACE))\n",
    "tf.io.gfile.makedirs(WORKSPACE)\n",
    "\n",
    "time_start = datetime.utcnow() \n",
    "print(\"Running preproc pipeline...\")\n",
    "run_preproc_pipeline(args)\n",
    "print(\"Pipeline is done.\")\n",
    "time_end = datetime.utcnow() \n",
    "time_elapsed = time_end - time_start\n",
    "print(\"Execution elapsed time: {} seconds\".format(time_elapsed.total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cooc-00000-of-00001.tfrecords  info.log  vocab-0.txt  vocab-1.txt\n"
     ]
    }
   ],
   "source": [
    "!ls {COOC_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min: 1\n",
      "P: 1000209\n",
      "max: 5\n"
     ]
    }
   ],
   "source": [
    "!head {COOC_DIR}/info.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Read TFRecords using tf.data APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_fn(file_pattern, batch_size):\n",
    "    \n",
    "    features = {\n",
    "        'item1': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'item2': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "        'score': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "        'weight': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "        'type': tf.io.FixedLenFeature(dtype=tf.string, shape=())\n",
    "    }\n",
    "\n",
    "    def _input_fn():\n",
    "        dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "            file_pattern,\n",
    "            batch_size,\n",
    "            features,\n",
    "            reader=tf.data.TFRecordDataset,\n",
    "            label_key=None,\n",
    "            num_epochs=1,\n",
    "            shuffle=True\n",
    "        )\n",
    "        return dataset\n",
    "    \n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-23 15:26:33.097090: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2021-09-23 15:26:33.250269: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Record 1:\n",
      "-item1:[b'53' b'45' b'26' b'53' b'65']\n",
      "-item2:[b'2581' b'532' b'1101' b'2956' b'3409']\n",
      "-score:[5. 2. 4. 4. 3.]\n",
      "-type:[b'P' b'P' b'P' b'P' b'P']\n",
      "-weight:[1. 1. 1. 1. 1.]\n",
      "\n",
      "Record 2:\n",
      "-item1:[b'65' b'48' b'35' b'53' b'10']\n",
      "-item2:[b'2431' b'923' b'2100' b'326' b'248']\n",
      "-score:[5. 4. 4. 5. 5.]\n",
      "-type:[b'P' b'P' b'P' b'P' b'P']\n",
      "-weight:[1. 1. 1. 1. 1.]\n",
      "\n",
      "Record 3:\n",
      "-item1:[b'6' b'62' b'17' b'48' b'5']\n",
      "-item2:[b'34' b'1883' b'235' b'1408' b'515']\n",
      "-score:[4. 2. 5. 4. 4.]\n",
      "-type:[b'P' b'P' b'P' b'P' b'P']\n",
      "-weight:[1. 1. 1. 1. 1.]\n",
      "\n",
      "Record 4:\n",
      "-item1:[b'11' b'53' b'19' b'25' b'23']\n",
      "-item2:[b'1732' b'2186' b'3596' b'3114' b'2021']\n",
      "-score:[4. 5. 5. 4. 2.]\n",
      "-type:[b'P' b'P' b'P' b'P' b'P']\n",
      "-weight:[1. 1. 1. 1. 1.]\n",
      "\n",
      "Record 5:\n",
      "-item1:[b'24' b'10' b'19' b'69' b'48']\n",
      "-item2:[b'3000' b'1513' b'3623' b'1721' b'1777']\n",
      "-score:[4. 3. 2. 4. 3.]\n",
      "-type:[b'P' b'P' b'P' b'P' b'P']\n",
      "-weight:[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# tf.enable_eager_execution()\n",
    "\n",
    "DATA_FILES = \"{}/cooc-*\".format(COOC_DIR)\n",
    "\n",
    "dataset = make_input_fn(DATA_FILES, batch_size=5)()\n",
    "for i, features in enumerate(dataset.take(5)):\n",
    "    print()\n",
    "    print(\"Record {}:\".format(i+1))\n",
    "    for key in features:\n",
    "        print(\"-{}:{}\".format(key, features[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-6.m79",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m79"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
